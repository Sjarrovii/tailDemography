{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data (Part 1)\n",
    "The purpose of this notebook is to read in raw excel data for multiple years, append them into a single dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Resume Here](#ResumeHere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Setting up Python](#SettingUp)\n",
    "    \n",
    "    a. [Setting the Location](#SettingLoc)\n",
    "    \n",
    "    b. [Importing Necessary Packages](#ImportingPackages)\n",
    "    \n",
    "    c. [Preparing for a Save](#PreparingSave)\n",
    "    \n",
    "\n",
    "2. [Handling Columns](#HandlingColumns)\n",
    "    \n",
    "    a. [Find Unique Column Names](#FindUniqueCol)\n",
    "    \n",
    "    b. [Eliminate Unnecessary Columns](#DropCol)\n",
    "    \n",
    "    c. [Combine Synonyms](#CombineCol)\n",
    "\n",
    "3. [Reading and Appending Data](#ReadingAppendingData)\n",
    "\n",
    "4. [Exporting Data](#ExportingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SettingUp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Python\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting the Location](#SettingLoc)\n",
    "    \n",
    "[Importing Necessary Packages](#ImportingPackages)\n",
    "    \n",
    "[Getting Data](#GettingData)\n",
    "    \n",
    "[Preparing for a Save](#PreparingSave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImportingPackages'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Packages\n",
    "\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting Up Python](#SettingUp)\n",
    "\n",
    "Here we import necessary packages. \n",
    "This chunk may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob,os\n",
    "from liz_number import lizsort,mindate,smallest,validate\n",
    "from liz_toes import make_str,label_pattern, replace_pattern,report_pattern\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_config_file(world_readable=True)\n",
    "\n",
    "# increase print limit\n",
    "pd.options.display.max_rows = 99999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PreparingSave'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for a Save\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting up Python](#SettingUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SettingLoc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Location\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting Up Python](#SettingUp)\n",
    "\n",
    "These chunks identify the locations from which we can get data and to which we can save data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data\n",
    "Raw data can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sourceDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "sourceDataBig = 'S:/Chris/TailDemography/TailDemography/Raw Data'\n",
    "# sourceBlack = 'C:/Users/test/Desktop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change the working directory to the source path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sourceDataBig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data\n",
    "The cleaned data will be saved to one of these locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "outputBig = 'S:/Chris/TailDemography/TailDemography/Cleaned Combined Data'\n",
    "# outputBlack = 'C:/Users/test/Desktop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HandlingColumns'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "We don't have to look in the multiple-sheet file.  It's clear that we'll have to identify a common set of columns prior to combining these files.  Let's define a few functions to help us do this.\n",
    "\n",
    "We will want to do the following:\n",
    "1. [Find Unique Column Names](#FindUniqueCol)\n",
    "2. [Eliminate Unnecessary Columns](#DropCol)\n",
    "3. [Combine Synonyms](#CombineCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use search the source path to locate and eventually read the raw data into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC 2000-03-modified from CC-SJ 00-03 final.xls',\n",
       " 'CC 2004.xlsx',\n",
       " 'CC 2015 - captures.xls',\n",
       " 'CC 2016 - captures.xls',\n",
       " 'CC 2017 Lizards - 3viii17 captures and obs.xls',\n",
       " 'xCC2005x.xls',\n",
       " 'xCC2006x.xls',\n",
       " 'xCC2007x.xls',\n",
       " 'xCC2008x.xls',\n",
       " 'xCC2009x.xls',\n",
       " 'xCC2010x.xlsx',\n",
       " 'xCC2011x.xls',\n",
       " 'xCC2012x.xls',\n",
       " 'xCC2013x.xls',\n",
       " 'xCC2014x.xlsx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles = glob.glob('*.xls*')\n",
    "rawfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect these data.  We will begin by looking at the number of columns and rows. In each file. To do this first we define a function, *xlsheets*, which we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlsheets(file):\n",
    "    \"\"\"The function takes a string, *file*, and returns the number and names of sheets in that file.\\\n",
    "    The function relies on the pandas package.\"\"\"\n",
    "    tmp = pd.ExcelFile(file)\n",
    "    res = {'file_name':file,'num_sheets':len(tmp.sheet_names),'names': tmp.sheet_names}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that when we call *xlsheets* on the first file in our list of files, it returns the total number of sheets and names of each sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'CC 2000-03-modified from CC-SJ 00-03 final.xls',\n",
       " 'num_sheets': 4,\n",
       " 'names': ['2000', '2001', '2002', '2003']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlsheets(rawfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply this to all of the files in raw files to inspect the data at the highest level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'file_name': 'CC 2000-03-modified from CC-SJ ...\n",
       "1     {'file_name': 'CC 2004.xlsx', 'num_sheets': 1,...\n",
       "2     {'file_name': 'CC 2015 - captures.xls', 'num_s...\n",
       "3     {'file_name': 'CC 2016 - captures.xls', 'num_s...\n",
       "4     {'file_name': 'CC 2017 Lizards - 3viii17 captu...\n",
       "5     {'file_name': 'xCC2005x.xls', 'num_sheets': 1,...\n",
       "6     {'file_name': 'xCC2006x.xls', 'num_sheets': 1,...\n",
       "7     {'file_name': 'xCC2007x.xls', 'num_sheets': 2,...\n",
       "8     {'file_name': 'xCC2008x.xls', 'num_sheets': 1,...\n",
       "9     {'file_name': 'xCC2009x.xls', 'num_sheets': 1,...\n",
       "10    {'file_name': 'xCC2010x.xlsx', 'num_sheets': 1...\n",
       "11    {'file_name': 'xCC2011x.xls', 'num_sheets': 1,...\n",
       "12    {'file_name': 'xCC2012x.xls', 'num_sheets': 1,...\n",
       "13    {'file_name': 'xCC2013x.xls', 'num_sheets': 1,...\n",
       "14    {'file_name': 'xCC2014x.xlsx', 'num_sheets': 1...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelayout = pd.Series(rawfiles).apply(xlsheets)\n",
    "filelayout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>num_sheets</th>\n",
       "      <th>sheetnames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC 2000-03-modified from CC-SJ 00-03 final.xls</td>\n",
       "      <td>4</td>\n",
       "      <td>[2000, 2001, 2002, 2003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC 2004.xlsx</td>\n",
       "      <td>1</td>\n",
       "      <td>[2004 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC 2015 - captures.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC 2016 - captures.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2016]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC 2017 Lizards - 3viii17 captures and obs.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2017]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xCC2005x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2005]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xCC2006x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xCC2007x.xls</td>\n",
       "      <td>2</td>\n",
       "      <td>[Sheet1, 2007]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xCC2008x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xCC2009x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[2009]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xCC2010x.xlsx</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sheet1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xCC2011x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sheet1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xCC2012x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xCC2013x.xls</td>\n",
       "      <td>1</td>\n",
       "      <td>[CC 2013 data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xCC2014x.xlsx</td>\n",
       "      <td>1</td>\n",
       "      <td>[2014]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         file_name  num_sheets  \\\n",
       "0   CC 2000-03-modified from CC-SJ 00-03 final.xls           4   \n",
       "1                                     CC 2004.xlsx           1   \n",
       "2                           CC 2015 - captures.xls           1   \n",
       "3                           CC 2016 - captures.xls           1   \n",
       "4   CC 2017 Lizards - 3viii17 captures and obs.xls           1   \n",
       "5                                     xCC2005x.xls           1   \n",
       "6                                     xCC2006x.xls           1   \n",
       "7                                     xCC2007x.xls           2   \n",
       "8                                     xCC2008x.xls           1   \n",
       "9                                     xCC2009x.xls           1   \n",
       "10                                   xCC2010x.xlsx           1   \n",
       "11                                    xCC2011x.xls           1   \n",
       "12                                    xCC2012x.xls           1   \n",
       "13                                    xCC2013x.xls           1   \n",
       "14                                   xCC2014x.xlsx           1   \n",
       "\n",
       "                  sheetnames  \n",
       "0   [2000, 2001, 2002, 2003]  \n",
       "1                    [2004 ]  \n",
       "2                     [2015]  \n",
       "3                     [2016]  \n",
       "4                     [2017]  \n",
       "5                     [2005]  \n",
       "6                     [2006]  \n",
       "7             [Sheet1, 2007]  \n",
       "8                     [2008]  \n",
       "9                     [2009]  \n",
       "10                  [Sheet1]  \n",
       "11                  [Sheet1]  \n",
       "12                    [data]  \n",
       "13            [CC 2013 data]  \n",
       "14                    [2014]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelayout = pd.DataFrame([list(file.values()) for file in filelayout])\n",
    "filelayout.columns = ['file_name','num_sheets','sheetnames']\n",
    "filelayout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to treat the 1st and 8th files in the source data differently when we read in the data.  Let's group the file names accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawfiles_ms = [rawfiles[0],rawfiles[7]]\n",
    "rawfiles_ss = list(set(rawfiles)- set(rawfiles_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of files with multiple sheets are now in the variable *rawfiles_ms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC 2000-03-modified from CC-SJ 00-03 final.xls', 'xCC2007x.xls']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of files with a single sheet are now in the variable *rawfiles_ss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC 2015 - captures.xls',\n",
       " 'xCC2006x.xls',\n",
       " 'CC 2016 - captures.xls',\n",
       " 'xCC2013x.xls',\n",
       " 'xCC2005x.xls',\n",
       " 'CC 2017 Lizards - 3viii17 captures and obs.xls',\n",
       " 'xCC2009x.xls',\n",
       " 'xCC2014x.xlsx',\n",
       " 'xCC2008x.xls',\n",
       " 'CC 2004.xlsx',\n",
       " 'xCC2010x.xlsx',\n",
       " 'xCC2011x.xls',\n",
       " 'xCC2012x.xls']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the number of columns in each file. We'll start with the single sheet files, since this is the easiest.  We will define another function, *xlcolshape* to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlcolshape(file):\n",
    "    \"\"\"xlcolshape takes a file name as a string and returns the shape of the excel file\"\"\"\n",
    "    return pd.read_excel(file).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call this function on the first of the single-sheet files, we can see that it returns a tuple in the format ('number of rows', 'number of columns')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 19)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlcolshape(rawfiles_ss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply this funtion to the list of single-sheet files for our inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (241, 19)\n",
       "1     (163, 16)\n",
       "2     (103, 21)\n",
       "3     (106, 19)\n",
       "4     (202, 16)\n",
       "5     (798, 21)\n",
       "6     (162, 16)\n",
       "7      (97, 19)\n",
       "8     (134, 20)\n",
       "9     (479, 16)\n",
       "10     (99, 41)\n",
       "11     (64, 19)\n",
       "12     (85, 19)\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rawfiles_ss).apply(xlcolshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='FindUniqueCol'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Unique Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)\n",
    "\n",
    "We'll define a function to extract column names and convert them to an approved set.  We'll use that function to allow us to only add unique names to a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xluniquecol(file,sheet=0):\n",
    "    \"\"\"xluniquecol takes a string filename of an excel file and extracts the column names as a set.\"\"\"\n",
    "    return list(pd.read_excel(file,sheet_name=sheet).columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how xluniquecol works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Species',\n",
       " 'Toes',\n",
       " 'Date',\n",
       " 'Sex',\n",
       " 'SVL',\n",
       " 'TL',\n",
       " 'RTL',\n",
       " 'Mass',\n",
       " 'Paint Mark',\n",
       " 'Location',\n",
       " 'Meters',\n",
       " 'New/Recap',\n",
       " 'Painted',\n",
       " 'Misc.',\n",
       " 'Vial',\n",
       " 'Time',\n",
       " 'Unnamed: 16',\n",
       " 'Spotted',\n",
       " 'Mark']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xluniquecol(rawfiles_ss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an empty set, *uniquecols*, that will eventually contain the unique column names in all of the files.\n",
    "\n",
    "We will append the unique column names from each file to *uniquecols*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 'Toe 12',\n",
       " 'sex',\n",
       " 'painted or not',\n",
       " 'Unnamed: 17',\n",
       " 'Vial',\n",
       " 'paint mark',\n",
       " 'RTL',\n",
       " 'Unnamed: 19',\n",
       " 'TL (mm)',\n",
       " '2015 or earlier',\n",
       " 'meters',\n",
       " 'Misc.',\n",
       " 'RTL (mm)',\n",
       " 'misc',\n",
       " 'Date',\n",
       " 'species',\n",
       " 'Toe 3',\n",
       " 'Painted',\n",
       " 'Toe 20',\n",
       " 'Unnamed: 16',\n",
       " 'Toe 9',\n",
       " 'Year',\n",
       " 'mass (g)',\n",
       " 'Toe 1',\n",
       " 'Toe 2',\n",
       " 'misc/notes',\n",
       " 'New/Recap',\n",
       " 'mass',\n",
       " 'NEW/recap',\n",
       " 'Mass',\n",
       " 'Marked',\n",
       " 'Toe 5',\n",
       " 'Toe 11',\n",
       " 'Toe 15',\n",
       " 'Sex',\n",
       " 'Tail condition (1=intact; 2=autotomized; 3=regrown)',\n",
       " 'Toe 14',\n",
       " 'Toe 4',\n",
       " 'Species',\n",
       " 'date',\n",
       " ' painted or not',\n",
       " 'Collectors',\n",
       " 'TL',\n",
       " 'Toe 6',\n",
       " 'TIME',\n",
       " 'Toes',\n",
       " 'SVL',\n",
       " 'Spotted',\n",
       " 'location',\n",
       " 'painted',\n",
       " 'VIAL',\n",
       " 'Toe 13',\n",
       " 'Toe 16',\n",
       " 'Mark',\n",
       " 'Toe 19',\n",
       " 'Paint Mark',\n",
       " 'TOES',\n",
       " 'Toe 10',\n",
       " 'Years Alive (known)',\n",
       " 'Toe 8',\n",
       " 'Toe 7',\n",
       " 'Meters',\n",
       " 'SVL (mm)',\n",
       " 'Toe 17',\n",
       " 'Toe 18',\n",
       " 'Location',\n",
       " 'Time',\n",
       " '1st Capture (year)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(rawfiles_ss).apply(xluniquecol)\n",
    "uniquecols = list()\n",
    "for u in tmp:\n",
    "    uniquecols = uniquecols+u\n",
    "uniquecols = list(set(uniquecols))\n",
    "uniquecols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a similar logic but loop within each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function xluniquecol in module __main__:\n",
      "\n",
      "xluniquecol(file, sheet=0)\n",
      "    xluniquecol takes a string filename of an excel file and extracts the column names as a set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xluniquecol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(filelayout.loc[filelayout.file_name==tmpfile].num_sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65DDFC0>\n",
      "<generator object <genexpr> at 0x00000281B65F6830>\n",
      "<generator object <genexpr> at 0x00000281B65DDFC0>\n"
     ]
    }
   ],
   "source": [
    "for file in filelayout.file_name:\n",
    "    tmpfile = file\n",
    "    tmprange = range(0,(int(filelayout.loc[filelayout.file_name==tmpfile].num_sheets)-1))\n",
    "    tmpsheet=filelayout.loc[filelayout.file_name==tmpfile,'sheetnames']\n",
    "    print(i for i in tmprange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sj',\n",
       " ' 1-13-19',\n",
       " datetime.datetime(2000, 3, 17, 0, 0),\n",
       " 'f',\n",
       " 52,\n",
       " 74,\n",
       " 0,\n",
       " 4.2,\n",
       " 'r1c',\n",
       " '1falls',\n",
       " 'NEW',\n",
       " 'Unnamed: 11',\n",
       " 'Unnamed: 12']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xluniquecol(filelayout.file_name[0],sheet=filelayout.sheetnames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp2 = pd.Series(rawfiles_ms).apply(xluniquecol)\n",
    "uniquecols = list()\n",
    "for u in tmp:\n",
    "    uniquecols = uniquecols+u\n",
    "uniquecols = list(set(uniquecols))\n",
    "uniquecols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DropCol'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate Unnecessary Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Cleaning Data](#CleaningData)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)\n",
    "\n",
    "Now we will try to identify unnecessary columns and eliminate them. Much of this will be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepCol = ['species', 'date', 'sex', 'svl', 'tl', 'rtl', 'mass',\n",
    "       'paint.mark', 'location', 'meters', 'new.recap', 'painted', 'misc',\n",
    "       'vial', 'autotomized', 'sighting', 'toes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pd.Series(keepCol).str.lower())-set(pd.Series(uniquecols).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pd.Series(uniquecols).str.lower())-set(pd.Series(keepCol).str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data for years 2000-2003 are contained in the same Excel file we will have to treat this file differently than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='CombineCol'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Synonymous Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Cleaning Data](#CleaningData)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have identified the columns we need to keep, we'll need to apply this list to the files as they are read into python by doing the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to identify potential synonyms. One approach is to take a column name that we want to keep and search through a list of names to identify any that contain enough of that name to be considered potentially synonymous.  Let's try to return these potential matches as values in a dictionary and return the desired name as the key for those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colmatchtodict(x,series, dictsource, key= None):\n",
    "    \"\"\"This takes a string, x, and a looks for values in a series that match that contain that string.\n",
    "    Those values which match are returned as values in a python dict for the key, key.\"\"\"\n",
    "    \n",
    "    assert isinstance(series,pd.Series)\n",
    "    if key is None:\n",
    "        key = x\n",
    "    tmp = series[series.astype(str).str.contains(x,case = False)].tolist()\n",
    "    dictsource[key] = tmp\n",
    "    return dictsource\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmatchtodict('toes',pd.Series(uniquecols),coldict, key = 'toes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happened when we apply this funtion to our, keepCol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(keepCol).apply(lambda x: colmatchtodict(x=x,series=pd.Series(uniquecols),dictsource=coldict))\n",
    "coldict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will manually adjust the values for 'tl'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict['tl']=['TL (mm)', 'TL', 'tl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use this dict to relabel the columns we wish to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that will take a value in *uniquecols*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsyn (name,dictionary, verbose = True):\n",
    "    \"\"\"\n",
    "    *findsyn* checks searches the values of the dict *dictionary* for the string, *name* and returns \n",
    "    the key for the key,value pair to which *name* belongs.\n",
    "    \"\"\"\n",
    "    tmp = pd.DataFrame({'preferredcol':list(dictionary.keys()),'synonymns':list(dictionary.values())})\n",
    "    try:\n",
    "        res = list(tmp.preferredcol[tmp.synonymns.apply(lambda x:name in x)])[0]\n",
    "    except:\n",
    "        res = None\n",
    "        if verbose == True:\n",
    "            print(\"No value matching \\\"{}\\\" was found in the dictionary.\".format(name))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is are a few examples of how *findsyn* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTi',coldict,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTi',coldict,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTL',coldict,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply *findsyn* to *uniquecol* and create a column of synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquecolsdf = pd.DataFrame({'uniquecols':uniquecols})\n",
    "uniquecolsdf['preferredcol'] = uniquecolsdf.uniquecols.apply(lambda x: findsyn(x,coldict,False))\n",
    "uniquecolsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will turn this dataframe back into a dict so that we can easily use it to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquecolsdf.index = uniquecolsdf.uniquecols\n",
    "uniquecolsdict = pd.Series(uniquecolsdf.preferredcol).to_dict()\n",
    "uniquecolsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dict, *uniquecolsdict* to rename the synonymous columns in our file....once we read them in,\n",
    "that is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ReadingAppendingData'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Appending Data\n",
    "[Top](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readnclean(x,dictionary,dtype=None):\n",
    "    \"\"\"\n",
    "    This function reads an excel file, renames columns deemed synonymous according to a dict,\n",
    "    *dictionary*, and drops unnecessary columns before returning the cleaner dataframe.\n",
    "    \"\"\"\n",
    "    tmp = pd.read_excel(x,dtype=dtype)\n",
    "    tmp.columns = pd.Series(tmp.columns).map(lambda x:dictionary[x])\n",
    "    dropidx =[None==col for col in list(tmp.columns)]\n",
    "    tmp=tmp.drop(columns=tmp.columns[dropidx])\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how *readnclean* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readnclean(rawfiles_ss[0],uniquecolsdict,dtype=str).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a df, *df*, with no data, but columns from our desired columns, *i.e.* the keys for coldict, as a placeholder to which we can append new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=coldict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read in all of the raw_ss files clean the column names and concatenate them into one large df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in rawfiles_ss:\n",
    "    df = pd.concat([df,readnclean(file,uniquecolsdict)],sort=True)\n",
    "    print(df.shape[0])\n",
    "print(\"\\n\\nFinal df has {} columns and {} rows.\".format(df.shape[1],df.shape[0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the *rawdata_ms* files we will apply a similar logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ExportingData'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data\n",
    "[Top](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
