{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data (Part 1)\n",
    "The purpose of this notebook is to read in raw excel data for multiple years, rename and trim columns, append cleaned files into a single dataframe and export this dataframe as an excel file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Setting up Python](#Setting-up-Python)\n",
    "    \n",
    "    1. [Setting the Location](#Setting-the-Location)\n",
    "    \n",
    "    2. [Importing Necessary Packages](#Importing-Necessary-Packages)\n",
    "    \n",
    "    3. [Functions](#Functions)\n",
    "    \n",
    "    4. [Preparing for a Save](#Preparing-for-a-Save)  \n",
    "\n",
    "2. [Handling Columns](#Handling-Columns)\n",
    "    \n",
    "    1. [Find Unique Column Names](#Find-Unique-Column-Names)\n",
    "    \n",
    "    2. [Eliminate Unnecessary Columns](#Eliminate-Unnecessary-Columns)\n",
    "    \n",
    "    3. [Combine Synonyms](#Combine-Synonyms)\n",
    "\n",
    "3. [Reading and Appending Data](#Reading-and-Appending-Data)\n",
    "\n",
    "4. [Exporting Data](#Exporting-Data)\n",
    "5. [Where to Next](#Where-to-Next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Python\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting the Location](#SettingLoc)\n",
    "    \n",
    "[Importing Necessary Packages](#ImportingPackages)\n",
    "    \n",
    "[Getting Data](#GettingData)\n",
    "    \n",
    "[Preparing for a Save](#PreparingSave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Packages\n",
    "\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting Up Python](#SettingUp)\n",
    "\n",
    "Here we import necessary packages. \n",
    "This chunk may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import glob,os\n",
    "\n",
    "# increase print limit\n",
    "pd.options.display.max_rows = 99999\n",
    "pd.options.display.max_colwidth = 50\n",
    "pd.set_option('mode.sim_interactive', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "[Back to: Top](#TOC)\n",
    "\n",
    "[Back to: Setting Up Python](#SettingUp)\n",
    "\n",
    "1. [xlcolshape](#xlcolshape)\n",
    "\n",
    "2. [xluniquecol2](#xluniquecol2)\n",
    "\n",
    "3. [colmatchtodict](#colmatchtodict)\n",
    "\n",
    "4. [findsyn](#findsyn)\n",
    "\n",
    "5. [readnsplit](#readnsplit)\n",
    "\n",
    "6. [mapndrop](#mapndrop)\n",
    "\n",
    "7. [namefile](#namefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlcolshape(file, verbose = True):\n",
    "    \"\"\"xlcolshape takes a file name as a string and returns the shape of the excel file\"\"\"\n",
    "    assert isinstance(verbose,bool),\"'verbose' must be bool not,{}\".format(type(verbose))\n",
    "    dictionary = {}\n",
    "    for sheet in pd.ExcelFile(file).sheet_names:\n",
    "        try:\n",
    "            tmp = pd.read_excel(file,sheet_name =sheet).shape\n",
    "            dictentry = file+'_'+sheet\n",
    "            dictionary[dictentry] = tmp\n",
    "            if verbose == True:\n",
    "                print(\"Doing stuff you asked me to do for file \\'{}\\',sheet \\'{}\\' programmer person.\"\\\n",
    "                      .format(file, sheet))            \n",
    "        except:\n",
    "            print(\"This didn't work for file {}, sheet {}\".format(file, sheet))\n",
    "            \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xluniquecol2(file, header = 0, verbose=True):\n",
    "    tmp = []\n",
    "    for sheet in pd.ExcelFile(file).sheet_names:\n",
    "        if (('species' in pd.read_excel(file,sheet_name=sheet, header = header).columns)\\\n",
    "            or('Species' in pd.read_excel(file,sheet_name=sheet, header = header).columns)):\n",
    "            try:\n",
    "                tmp = list(set(tmp+list(pd.read_excel(file,sheet_name=sheet).columns)))\n",
    "                if verbose==True:\n",
    "                    print(\"Doing stuff you asked me to do for file \\'{}\\',sheet \\'{}\\' programmer person.\"\\\n",
    "                          .format(file,sheet))\n",
    "                res = tmp\n",
    "            except:\n",
    "                print(\"This didn't work for file {}, sheet {}\".format(file,sheet))\n",
    "        else:\n",
    "            print(\"Check columns for file {}.\".format(file))\n",
    "            res = None\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colmatchtodict(x,series, dictsource, key= None):\n",
    "    \"\"\"This takes a string, x, and a looks for values in a series that match that contain that string.\n",
    "    Those values which match are returned as values in a python dict for the key, key.\"\"\" \n",
    "    assert isinstance(series,pd.Series)\n",
    "    if key is None:\n",
    "        key = x\n",
    "    tmp = series[series.astype(str).str.contains(x,case = False)].tolist()\n",
    "    dictsource[key] = tmp\n",
    "    return dictsource\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsyn (name,dictionary, verbose = True):\n",
    "    \"\"\"\n",
    "    *findsyn* checks searches the values of the dict *dictionary* for the string, *name* and returns \n",
    "    the key for the key,value pair to which *name* belongs.\n",
    "    \"\"\"\n",
    "    tmp = pd.DataFrame({'preferredcol':list(dictionary.keys()),'synonymns':list(dictionary.values())})\n",
    "    try:\n",
    "        res = list(tmp.preferredcol[tmp.synonymns.apply(lambda x:name in x)])[0]\n",
    "    except:\n",
    "        res = None\n",
    "        if verbose == True:\n",
    "            print(\"No value matching \\\"{}\\\" was found in the dictionary.\".format(name))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readnsplit(file,newsourcefolder,dtype=None,verbose=True):\n",
    "    \"\"\"\n",
    "    This function reads an excel file, splits its sheets into separate files and saves them to folder\n",
    "    *newsourcefolder*.\n",
    "    \"\"\"\n",
    "    suffix = '.'+file.split('.')[1]\n",
    "    prefix = file[:-len(suffix)]\n",
    "    for sheet in pd.ExcelFile(file).sheet_names:\n",
    "        try:\n",
    "            splitfile = newsourcefolder+'/'+prefix+'_'+sheet+suffix\n",
    "            tmp = pd.read_excel(file,dtype=dtype, sheet_name=sheet).to_excel(splitfile,index=False)\n",
    "            if verbose==True:\n",
    "                print(\"Success!  \\'{}\\',sheet \\'{}\\' has been saved to {} and the corresponding\\\n",
    "                google drive file as {}.\".format(file,sheet,newsourcefolder,splitfile))\n",
    "            continue\n",
    "        except:\n",
    "            print(\"Unable to save \\'{}\\',sheet \\'{}\\' as a separate file.\".format(file,sheet))         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapndrop(df,dictionary,verbose=True):\n",
    "    \"\"\"\n",
    "    This function renames columns in *df* deemed synonymous according to a dict,\n",
    "    *dictionary*, and drops unnecessary columns before returning the cleaner dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.columns = pd.Series(df.columns).map(lambda x:dictionary[x])\n",
    "        tmp = df\n",
    "        if verbose==True:\n",
    "            print(\"Successfully mapped columns for df.\")\n",
    "        dropidx =[None==col for col in list(tmp.columns)]\n",
    "        tmp=tmp.drop(columns=df.columns[dropidx])\n",
    "        if verbose==True:\n",
    "            print(\"Successfully dropped unnecessary columns for df.\")\n",
    "    except:\n",
    "        tmp = None\n",
    "        print(\"Skipping mapndrop call for df.\")\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to: Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namefile(name, tzadjust=5,tzdirection = '-', adjprecision='minutes', filetype = 'csv'):\n",
    "    \"\"\"takes a filename and filetype, and adds a timestamp adjusted relative to gmt to a precision \n",
    "    and returns a string that concatenates them.\"\"\"\n",
    "    assert isinstance(name,str),\"'name' must be of type str.\"\n",
    "    assert isinstance(tzadjust,int),\"'tzadjust' must be of type int\"\n",
    "    assert adjprecision in ['date','hour','minutes','seconds', 'max'], \"'adjprecision' must be either \\\n",
    "    'date', hour','minutes','seconds', or 'max'\"\n",
    "    precision= {'max':None,'seconds':-7,'minutes':-9, 'hours':-14,'date':-20}\n",
    "    if tzdirection== '-':\n",
    "        timestamp = (pd.to_datetime('now')-pd.Timedelta(hours=tzadjust))\n",
    "    else:\n",
    "        timestamp = (pd.to_datetime('now')+pd.Timedelta(hours=int(tzadjust[1:])))\n",
    "    timestamp = str(timestamp).replace(':','hrs',1).replace(':','min',1)\n",
    "    timestamp = timestamp[:precision[adjprecision]]\n",
    "    filename = name+'_' + timestamp+ '.' +filetype\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for a Save\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting up Python](#SettingUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Location\n",
    "[Top](#TOC)\n",
    "\n",
    "[Setting Up Python](#SettingUp)\n",
    "\n",
    "These chunks identify the locations from which we can get data and to which we can save data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data\n",
    "Raw data can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sourceDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "# sourceDataBig = 'S:/Chris/TailDemography/TailDemography/Raw Data'\n",
    "# sourceBlack = 'C:/Users/test/Desktop'\n",
    "sourceGandolf = 'C:/Users/craga/Google Drive/TailDemography/Raw Data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate Source Data\n",
    "Intermediate files can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sourceInterDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/Intermediate Files/Source'\n",
    "# sourceinterDataBig = 'S:/Chris/TailDemography/TailDemography/Intermediate Files/Source'\n",
    "# sourceBlack = 'C:/Users/test/Desktop'\n",
    "sourceInterGandolf = 'C:/Users/craga/Google Drive/TailDemography/Intermediate Files/Source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change the working directory to the source path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sourceGandolf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data\n",
    "The cleaned data will be saved to one of these locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "# outputBig = 'S:/Chris/TailDemography/TailDemography/Cleaned Combined Data'\n",
    "# outputBlack = 'C:/Users/test/Desktop'\n",
    "outputGandolf = 'C:/Users/craga/Google Drive/TailDemography/Cleaned Combined Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputPers = 'C:/Users/Christopher/Google Drive/TailDemography/Files for review/Source files'\n",
    "# reviewfolderBig = 'S:/Chris/TailDemography/TailDemography/Files for review/Source files'\n",
    "reviewGandolf = 'C:/Users/craga/Google Drive/TailDemography/Files for review/Source files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "We don't have to look in the multiple-sheet file.  It's clear that we'll have to identify a common set of columns prior to combining these files.  Let's define a few functions to help us do this.\n",
    "\n",
    "We will want to do the following:\n",
    "1. [Find Unique Column Names](#FindUniqueCol)\n",
    "2. [Eliminate Unnecessary Columns](#DropCol)\n",
    "3. [Combine Synonyms](#CombineCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use search the source path to locate and eventually read the raw data into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC 2000-03-modified from CC-SJ 00-03 final-modified w headers-3Jan19.xlsx',\n",
       " 'CC 2004.xlsx',\n",
       " 'CC 2015 - captures.xls',\n",
       " 'CC 2016 - captures.xls',\n",
       " 'CC 2017 Lizards - 3viii17 captures and obs.xls',\n",
       " 'xCC2005x.xls',\n",
       " 'xCC2006x.xls',\n",
       " 'xCC2007x.xls',\n",
       " 'xCC2008x.xls',\n",
       " 'xCC2009x.xls',\n",
       " 'xCC2010x.xlsx',\n",
       " 'xCC2011x.xls',\n",
       " 'xCC2012x.xls',\n",
       " 'xCC2013x.xls',\n",
       " 'xCC2014x.xlsx']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles = glob.glob('*.xls*')\n",
    "rawfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll separate these into files with single or multiple sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawfiles_ms = [rawfiles[0],rawfiles[7]]\n",
    "rawfiles_ss = list(set(rawfiles)- set(rawfiles_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of files with multiple sheets are now in the variable *rawfiles_ms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC 2000-03-modified from CC-SJ 00-03 final-modified w headers-3Jan19.xlsx',\n",
       " 'xCC2007x.xls']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of files with a single sheet are now in the variable *rawfiles_ss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xCC2013x.xls',\n",
       " 'xCC2005x.xls',\n",
       " 'xCC2008x.xls',\n",
       " 'xCC2009x.xls',\n",
       " 'xCC2011x.xls',\n",
       " 'xCC2006x.xls',\n",
       " 'xCC2010x.xlsx',\n",
       " 'xCC2012x.xls',\n",
       " 'CC 2015 - captures.xls',\n",
       " 'CC 2004.xlsx',\n",
       " 'CC 2017 Lizards - 3viii17 captures and obs.xls',\n",
       " 'xCC2014x.xlsx',\n",
       " 'CC 2016 - captures.xls']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawfiles_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the number of columns in each file. We'll start with the single sheet files, since this is the easiest.  We will use the function, *xlcolshape* to make this easier. \n",
    "When we call this function on the first of the single-sheet files, we can see that it returns a tuple in the format ('number of rows', 'number of columns'). The code for *xlcolshape* can be found in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing stuff you asked me to do for file 'xCC2013x.xls',sheet 'CC 2013 data' programmer person.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xCC2013x.xls_CC 2013 data': (106, 20)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlcolshape(rawfiles_ss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply this function to the list of files for our inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'CC 2000-03-modified from CC-SJ 00-03 final-m...\n",
       "1                     {'CC 2004.xlsx_2004 ': (479, 16)}\n",
       "2            {'CC 2015 - captures.xls_2015': (241, 19)}\n",
       "3            {'CC 2016 - captures.xls_2016': (103, 21)}\n",
       "4     {'CC 2017 Lizards - 3viii17 captures and obs.x...\n",
       "5                      {'xCC2005x.xls_2005': (202, 17)}\n",
       "6                      {'xCC2006x.xls_2006': (163, 17)}\n",
       "7     {'xCC2007x.xls_Sheet1': (507, 16), 'xCC2007x.x...\n",
       "8                      {'xCC2008x.xls_2008': (134, 20)}\n",
       "9                      {'xCC2009x.xls_2009': (162, 16)}\n",
       "10                   {'xCC2010x.xlsx_Sheet1': (99, 41)}\n",
       "11                    {'xCC2011x.xls_Sheet1': (64, 19)}\n",
       "12                      {'xCC2012x.xls_data': (85, 19)}\n",
       "13             {'xCC2013x.xls_CC 2013 data': (106, 20)}\n",
       "14                     {'xCC2014x.xlsx_2014': (97, 19)}\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rawfiles).apply(lambda x: xlcolshape(x,verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Unique Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)\n",
    "\n",
    "We'll use the function, *xluniqucol2* to extract column names and convert them to an approved set.  We'll use that function to allow us to only add unique names to a list of column names. \n",
    "\n",
    "Here is an example of how xluniquecol2 works for a file with one sheet.  You can find the code for *xluniquecol2* in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVL',\n",
       " 'TL',\n",
       " 'Location',\n",
       " 'Mark',\n",
       " 'Painted',\n",
       " 'Species',\n",
       " 'Sex',\n",
       " 'Mass',\n",
       " 'Meters',\n",
       " 'Unnamed: 17',\n",
       " 'New/Recap',\n",
       " 'Misc.',\n",
       " 'Spotted',\n",
       " 'Paint Mark',\n",
       " 'Time',\n",
       " 'Toes',\n",
       " 'Date',\n",
       " 'RTL',\n",
       " 'Unnamed: 0',\n",
       " 'Vial']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xluniquecol2(rawfiles_ss[0],verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how xluniquecol2 works for a file with multiple sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xluniquecol2(rawfiles_ms[0],verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an empty set, *uniquecols2*, that will eventually contain the unique column names in all of the files.\n",
    "\n",
    "We will append the unique column names from each file to *uniquecols2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.Series(rawfiles).apply(xluniquecol2,verbose=False)\n",
    "uniquecols2 = list()\n",
    "for u in tmp:\n",
    "    uniquecols2 = uniquecols2+u\n",
    "uniquecols2 = list(set(uniquecols2))\n",
    "uniquecols2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate Unnecessary Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Cleaning Data](#CleaningData)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)\n",
    "\n",
    "Now we will try to identify unnecessary columns and eliminate them. Much of this will be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepCol = ['species', 'date', 'sex', 'svl', 'tl', 'rtl', 'mass',\n",
    "       'paint.mark', 'location', 'meters', 'new.recap', 'painted', 'misc',\n",
    "       'vial', 'autotomized', 'sighting', 'toes','filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pd.Series(keepCol).str.lower())-set(pd.Series(uniquecols2).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pd.Series(uniquecols2).str.lower())-set(pd.Series(keepCol).str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data for years 2000-2003 are contained in the same Excel file we will have to treat this file differently than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Synonymous Columns\n",
    "[Top](#TOC)\n",
    "\n",
    "[Cleaning Data](#CleaningData)\n",
    "\n",
    "[Handling Columns](#HandlingColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have identified the columns we need to keep, we'll need to apply this list to the files as they are read into python by doing the following:\n",
    "\n",
    "We will use a function, *colmatchtodict*,  to identify potential synonyms. Here's an example of how *colmatchtodict* works.  The code for this function can be found in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmatchtodict('toes',pd.Series(uniquecols2),coldict, key = 'toes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happened when we apply this funtion to our, keepCol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(keepCol).apply(lambda x: colmatchtodict(x=x,series=pd.Series(uniquecols2),dictsource=coldict))\n",
    "coldict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will manually adjust the values for 'tl' and 'filename'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldict['tl']=['TL (mm)', 'TL', 'tl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use this dict to relabel the columns we wish to keep.\n",
    "\n",
    "We will use the function, *findsyn* to identify potential synonymous to the columnlabels in *keepcols* among the column labels in *uniquecols2*. \n",
    "\n",
    "Here is are a few examples of how *findsyn* works.  The code can be found in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTi',coldict,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTi',coldict,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findsyn('RTL',coldict,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply *findsyn* to *uniquecol* and create a column of synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquecols2df = pd.DataFrame({'uniquecols2':uniquecols2})\n",
    "uniquecols2df['preferredcol'] = uniquecols2df.uniquecols2.apply(lambda x: findsyn(x,coldict,False))\n",
    "uniquecols2df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will turn this dataframe back into a dict so that we can easily use it to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquecols2df.index = uniquecols2df.uniquecols2\n",
    "uniquecols2dict = pd.Series(uniquecols2df.preferredcol).to_dict()\n",
    "uniquecols2dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dict, *uniquecols2dict* to rename the synonymous columns in our file....once we read them in,\n",
    "that is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Appending Data\n",
    "[Top](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the function *readnsplit* to actually read in the source files, drop unnecessary columns and renaming columns according to a dictionary. \n",
    "\n",
    "Here is an example of how *readnsplit* works.  The code can be found in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "readnsplit(rawfiles[0],sourceinterDataBig,str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in rawfiles:\n",
    "    readnsplit(file,sourceInterGandolf,dtype=str, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the directory to the location where the intermediate files this operates on can be found.  We will also save a list of the files names in that location for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sourceInterGandolf)\n",
    "splitfiles = glob.glob('*xls*')\n",
    "splitfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we remove 'xCC2007x_Sheet1.xls' from the list of files we will process intermediate files since this is a subset of the 'xCC2007x_2007.xls' reordered and with some columns dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitfiles = list(set(splitfiles)-set(['xCC2007x_Sheet1.xls']))\n",
    "splitfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the function *mapndrop* to drop unnecessary columns and renaming columns according to a dictionary.\n",
    "\n",
    "Here are a few examples of how *mapndrop* works.  The code can be found in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapndrop(df=pd.read_excel(splitfiles[0],dtype=str),dictionary=uniquecols2dict,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapndrop(df=pd.read_excel(splitfiles[4],dtype=str),dictionary=uniquecols2dict, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a df, *df*, with no data, but columns from our desired columns, *i.e.* the keys for coldict, as a placeholder to which we can append new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=coldict.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read in all of the successfully split files, clean the column names, and concatenate them into one large df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in splitfiles:\n",
    "    df = pd.concat([df,mapndrop(pd.read_excel(file,dtype=str),uniquecols2dict)],sort=True)\n",
    "    print(df.shape[0])\n",
    "print(\"\\n\\nFinal df has {} columns and {} rows.\".format(df.shape[1],df.shape[0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(['species', 'toes', 'sex', 'date', 'svl', 'tl', 'rtl', 'autotomized', 'mass', \n",
    "                 'location', 'meters', 'new.recap', 'painted', 'sighting', \n",
    "                 'paint.mark', 'vial', 'misc'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data\n",
    "[Top](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the function, *namefile*, to create a timestamped name for file to be exported.  You can find the code for *namefile* in [Functions](#functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = namefile('Appended and Trimmed CC Data 2000-2017')\n",
    "os.chdir(outputGandolf)\n",
    "df.to_csv(filename,index = False)\n",
    "print(\"\\'{}\\' has been saved to \\'{}\\' and the corresponding drive google drive location.\"\\\n",
    "      .format(filename, outputGandolf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to Next\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "Next proceed to Cleaning CC (Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
