{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Things to Do](#Things-to-Do)\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Set up Python](#Set-up-Python)\n",
    "2. [Functions](#Functions)\n",
    "3. [Getting Data](#Get-Data)\n",
    "4. [Analyze Data](#Analyze-Data)\n",
    "5. [Export Files](#Export-Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Do\n",
    "\n",
    "\n",
    "- [Resume Here](#Resume-Here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook contains code and output of descriptive analyses for the 2000-2017 CC dataset after cleaning.\n",
    "\n",
    "The objectives of this notebook are to:\n",
    "\n",
    "The metrics we examine are: .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set up Python\n",
    "\n",
    "First we will need to set up the python environment, importing the necessary packages and setting the display options.\n",
    "\n",
    "[Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, logging\n",
    "from summary_functions import *\n",
    "from scipy import stats\n",
    "from monthlit import *\n",
    "from prettyprint import *\n",
    "\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "# plotly.tools.set_config_file(world_readable=True)\n",
    "\n",
    "\n",
    "# increase print limit\n",
    "pd.options.display.max_rows = 99999\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceDict = {'dataBig':{'source':'S:/Chris/TailDemography/TailDemography/weather data files'\n",
    "                         ,'log':'S:/Chris/TailDemography/TailDemography/weather data files/logs'\n",
    "                         ,'output':'S:/Chris/TailDemography/TailDemography/weather data files/outputFiles/'},\n",
    "              'silverSurfer':{'source':'C:\\\\Users\\\\craga_eowcrpe\\\\Google Drive\\\\TailDemography\\\\weather data files/outputFiles'\n",
    "                              ,'log':'C:\\\\Users\\\\craga_eowcrpe\\\\Google Drive\\\\TailDemography\\\\weather data files/logs'\n",
    "                              ,'output':'C:\\\\Users\\\\craga_eowcrpe\\\\Google Drive\\\\TailDemography\\\\weather data files/outputFiles'}\n",
    "              ,'dataPers':{'source':'C:/Users/Christopher/Google Drive/TailDemography/weather data files'\n",
    "                           ,'log': 'C:\\\\Users\\\\craga_eowcrpe\\\\Google Drive\\\\TailDemography\\\\weather data files/logs'\n",
    "                           ,'output':'C:/Users/Christopher/Google Drive/TailDemography/weather data files/outputFiles'}\n",
    "             ,'gandolf':{'source':'C:/Users/craga/Google Drive/TailDemography/weather data files'\n",
    "                           ,'log': 'C:/Users/craga/Google Drive/TailDemography/weather data files/logs'\n",
    "                           ,'output':'C:/Users/craga/Google Drive/TailDemography/weather data files/outputFiles'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:/Users/craga/Google Drive/TailDemography/weather data files',\n",
       " 'log': 'C:/Users/craga/Google Drive/TailDemography/weather data files/logs',\n",
       " 'output': 'C:/Users/craga/Google Drive/TailDemography/weather data files/outputFiles'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = deviceDict['gandolf']\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=device['log']+'Desriptive Analyses.log'\n",
    "                    , filemode='a',\n",
    "                    format='%(funcName)s - %(levelname)s - %(message)s - %(asctime)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "This section contains functions that were created for this notebook.\n",
    "\n",
    "- [distribution](#distribution) #delete this we will use scipy stats describe instead\n",
    "- [monthlit](#monthlit)\n",
    "- [description](#description)\n",
    "- [vocab_run](#vocab_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribution\n",
    "[Back to Top](#TOC)\n",
    "\n",
    "[Back to Functions](#Functions)\n",
    "\n",
    "*distribution* takes a series or list of numeric objects, *x*, and returns descriptive stats of x including\n",
    "        n, minimum, maximum, median, sIQR, mean, and stdev\n",
    "    \n",
    "Here are a few examples of how *distribution* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = [0,1,2,'r']\n",
    "distribution(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>median</th>\n",
       "      <th>siqr</th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  minimum  maximum  median  siqr  mean  stdev\n",
       "0  3        0        2     1.0   0.5   1.0    1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar = [0,1,2]\n",
    "distribution(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monthlit\n",
    "[Back to Top](#TOC)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of how _monthlit_ works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dates\n",
       "0 2018-12-09\n",
       "1 2019-08-05\n",
       "2 2017-07-04\n",
       "3        NaT\n",
       "4        NaT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.DataFrame(data={'dates':['2018-12-9','2019-8-5', '2017/7/4',np.nan,None]})\n",
    "dates.dates = pd.to_datetime(dates.dates)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlit(dates.dates.dt.month[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates.dates.dt.month.apply(monthlit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## description\n",
    "[Back to Top](#TOC)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(x,variable,percentage=False):\n",
    "    if percentage:\n",
    "            res = x[variable].describe()\n",
    "            res[['mean','std','min','25%','50%','75%','max']] = res[['mean','std','min','25%','50%','75%','max']]\\\n",
    "            .apply(lambda x:x*100) \n",
    "#Need to Add CI calculation to this function\n",
    "#             meanCI = 'not calculated'\n",
    "    else:\n",
    "        res = x[variable].describe() \n",
    "    res['siqr'] = (res['75%']-res['25%'])/2\n",
    "    res['meanCI'] = 'not calculated'\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocab_run\n",
    "[Back to Top](#TOC)\n",
    "\n",
    "[Back to Functions](#Functions)\n",
    "\n",
    "*vocab_run* takes a list, joins its the first the elements with a separator placing a different separator between\n",
    "     the penultimate and final members of the list and returns the result as a string\n",
    "     :param x: a list of strings to be concatenated\n",
    "     :param connector_dict: a dictionary with keys describing the size of the list and values indicating the type of\n",
    "     connectors separate the list elements.\n",
    "    \n",
    "Here are a few examples of how *vocab_run* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Could you bring some {} please?\".format(vocab_run(['foo','bar','stuffkins'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You can either have {}.  You'll have to make a choice.\"\\\n",
    "      .format(vocab_run(['foo','bar','stuffkins'],connector_dict={1: None, 2: ' or ', 'run': ', '})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll display all files in the source folder with the prefix _'cleaned CC data 2000-2017'_. The file names will be saved in a variable, _mysourcefiles_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "[Top](#TOC)\n",
    "\n",
    "Here we can set the locations from which we get data and to which we export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(device['source'])\n",
    "mysourcefiles = glob.glob('*_weather*.csv')\n",
    "mysourcefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweatherdata(afile,sourcename):\n",
    "    tmp = pd.read_csv(afile)\n",
    "    tmp['source'] = sourcename\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([getweatherdata(afile,afile.split('_')[0]) for afile in mysourcefiles]).drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop = pd.read_csv('C:/Users/craga/Google Drive/TailDemography/outputFiles/Descriptive/population size.csv')\n",
    "df_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Data\n",
    "[Top](#TOC)\n",
    "\n",
    "We will first examine the range and distribution of number of variables in our data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather for a year includes weather since the last collection date of the previous calendar year \n",
    "seasons={'Dec':'winter','Jan':'winter','Feb':'winter',\n",
    "         'Mar':'spring','Apr':'spring','May':'spring',\n",
    "         'Jun':'summer','Jul':'summer','Aug':'summer',\n",
    "         'Sept':'fall','Oct':'fall','Nov':'fall'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split analysis up:\n",
    "- Analysis 1\n",
    "    - weather in the previous 365 days relative to the first date of collection/sighting for the current calendar year\n",
    "    - additional factor would be population for previous calendar year (year -1)\n",
    "- Analysis 2 (Skip this for now)\n",
    "    - weather in the previous 365 days relative to the first date of collection/sighting for the current calendar year\n",
    "    - additonal factor would be populationi in the the current calendar year (year 0)\n",
    "    - dv: population in (year 1 through year x)\n",
    "- Analysis 3,4,5\n",
    "    - IV\n",
    "        - population in year -1\n",
    "        - onset of monsoon in year 0\n",
    "        - precipitation in summer\n",
    "        - interaction ?\n",
    "    - DV\n",
    "        - population in year 1\n",
    "        - age/size structure in year 1 (looking for 45mm to 65mm)\n",
    "        - sex ratio in year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could be used to generate season-level weather data (use season dates) - Chris\n",
    "# This could also be used to approximate the start of the monsoon season\n",
    "# Check historical data in May and June to identify in notes when the first juvenile were spotted - (George and Chris)\n",
    "## Look for correlates in the data\n",
    "# Use SWRS data to identify start of monsoons (George to get SWRS data)\n",
    "# what other precipitation and temperature in the NOAA data set have been used for this (George and Chris to check the lit)\n",
    "df['month'] = df.month.apply(monthlit)\n",
    "df['season'] = df.month.apply(lambda x: seasons[x])\n",
    "df_season = pd.DataFrame(df.groupby(['source','year','season'])['PRCP','SNOW','TMAX','TMIN','TAVG'].describe())[1:-1]\n",
    "df_season.columns = [' '.join(col).strip() for col in df_season.columns.values]\n",
    "df_season = df_season.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_season['year-season'] = df_season.year.astype(str) + '-' + df_season.season\n",
    "df_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_annual = pd.DataFrame(df.groupby(['source','year'])['PRCP','SNOW','TMAX','TMIN','TAVG'].describe())[1:-1]\n",
    "df_annual.columns = [' '.join(col).strip() for col in df_annual.columns.values]\n",
    "df_annual = df_annual.reset_index().sort_values('year')\n",
    "df_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we predict the change in population size using the prvious year's weather?\n",
    "First let's make a new data set that will allow us to vizualize the potential relationship between precipitation and population size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_annual = df_annual.merge(df_pop.loc[df_pop.sex=='f'].drop(columns=['propMale','sex','liznumber']),on = ['year'],how='left')\n",
    "df_reg_annual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_season = df_season.merge(df_pop.loc[df_pop.sex=='f'].drop(columns=['propMale','sex','liznumber']),on = ['year'],how='left')\n",
    "df_reg_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop paradise\n",
    "df_reg_annual['popinYearless1'] = df_reg_annual.groupby('source').liznumberYear.shift(-1)\n",
    "df_reg_annual['popinYearless2'] = df_reg_annual.groupby('source').liznumberYear.shift(-2)\n",
    "df_reg_annual['popinYearless3'] = df_reg_annual.groupby('source').liznumberYear.shift(-3)\n",
    "df_reg_annual['popinYearless4'] = df_reg_annual.groupby('source').liznumberYear.shift(-4)\n",
    "df_reg_annual['popinYearless5'] = df_reg_annual.groupby('source').liznumberYear.shift(-5)\n",
    "df_reg_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop paradise\n",
    "df_reg_season['popinYearless1'] = df_reg_season.groupby('source').liznumberYear.shift(-1)\n",
    "df_reg_season['popinYearless2'] = df_reg_season.groupby('source').liznumberYear.shift(-2)\n",
    "df_reg_season['popinYearless3'] = df_reg_season.groupby('source').liznumberYear.shift(-3)\n",
    "df_reg_season['popinYearless4'] = df_reg_season.groupby('source').liznumberYear.shift(-4)\n",
    "df_reg_season['popinYearless5'] = df_reg_season.groupby('source').liznumberYear.shift(-5)\n",
    "df_reg_season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate(m,dv,placement=(1,1)):\n",
    "    assert(dv in m.columns)\n",
    "    return m[dv].sort_values().reset_index().iloc[placement[0]:placement[1]+1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topcorr(corrdf,lowestrank,dvs):\n",
    "    candidates = [candidate(corrdf,dv,(1,lowestrank)) for dv in dvs]\n",
    "    merger =  reduce(lambda x, y: pd.merge(x, y, on = 'index', how = 'outer'), candidates).fillna('--')\n",
    "    return merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Dropping proportion of Females, but will put it back once I can order the y-axis\n",
    "corrPortal_annual = df_reg_annual.loc[(df_reg_annual.source=='portal')]\\\n",
    ".drop(columns=['PRCP count', 'SNOW count', 'TMAX count', 'TMIN count', 'TAVG count','propFemale',\n",
    "              'SNOW min', 'SNOW 25%', 'SNOW 50%',]).corr()\n",
    "testx = corrPortal_annual.columns\n",
    "testy = corrPortal_annual.index\n",
    "testz = corrPortal_annual.values\n",
    "test = go.Figure(go.Heatmap(x=testx,y=testy,z=testz))\n",
    "plot(test, filename = 'portal annual correlation matrix.html')\n",
    "iplot(test, filename = 'portal annual correlation matrix.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual =topcorr(corrPortal_annual,3,mydvs) \n",
    "annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do\n",
    "- Run MV correlation\n",
    "    - IV should be pop at year 0\n",
    "    - DV should be pop at year 1- year X plus abiotic factors\n",
    "    - Which abiotic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPortal_spring = df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['spring']))]\\\n",
    ".drop(columns=['PRCP count', 'SNOW count', 'TMAX count', 'TMIN count', 'TAVG count','propFemale',\n",
    "              'SNOW min', 'SNOW 25%', 'SNOW 50%',]).corr()\n",
    "testx = corrPortal_spring.columns\n",
    "testy = corrPortal_spring.index\n",
    "testz = corrPortal_spring.values\n",
    "test = go.Figure(go.Heatmap(x=testx,y=testy,z=testz))\n",
    "plot(test, filename = 'portal spring correlation matrix.html')\n",
    "iplot(test, filename = 'portal spring correlation matrix.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring= topcorr(corrPortal_spring,3,mydvs) \n",
    "spring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPortal_summer = df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['summer']))]\\\n",
    ".drop(columns=['PRCP count', 'SNOW count', 'TMAX count', 'TMIN count', 'TAVG count','propFemale',\n",
    "              'SNOW min', 'SNOW 25%', 'SNOW 50%',]).corr()\n",
    "testx = corrPortal_summer.columns\n",
    "testy = corrPortal_summer.index\n",
    "testz = corrPortal_summer.values\n",
    "test = go.Figure(go.Heatmap(x=testx,y=testy,z=testz))\n",
    "plot(test, filename = 'portal summer correlation matrix.html')\n",
    "iplot(test, filename = 'portal summer correlation matrix.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer= topcorr(corrPortal_summer,3,mydvs) \n",
    "summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPortal_fall = df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['fall']))]\\\n",
    ".drop(columns=['PRCP count', 'SNOW count', 'TMAX count', 'TMIN count', 'TAVG count','propFemale',\n",
    "              'SNOW min', 'SNOW 25%', 'SNOW 50%',]).corr()\n",
    "testx = corrPortal_fall.columns\n",
    "testy = corrPortal_fall.index\n",
    "testz = corrPortal_fall.values\n",
    "test = go.Figure(go.Heatmap(x=testx,y=testy,z=testz))\n",
    "plot(test, filename = 'portal fall correlation matrix.html')\n",
    "iplot(test, filename = 'portal fall correlation matrix.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall= topcorr(corrPortal_fall,3,mydvs) \n",
    "fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popvars = ['popinYearless1','popinYearless2','popinYearless3','popinYearless4']\n",
    "weathvars = fall['index'].tolist()\n",
    "for popvar in popvars:\n",
    "    for weathvar in weathvars:\n",
    "        r,p = stats.pearsonr(df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['fall']))]\\\n",
    "                             [popvar].dropna(),df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['fall']))]\\\n",
    "                             [weathvar].dropna())\n",
    "        print('{} vs {}: r={}; p={}'.format(popvar,weathvar,r,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPortal_winter = df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['winter']))]\\\n",
    ".drop(columns=['PRCP count', 'SNOW count', 'TMAX count', 'TMIN count', 'TAVG count','propFemale',\n",
    "              'SNOW min', 'SNOW 25%', 'SNOW 50%',]).corr()\n",
    "testx = corrPortal_winter.columns\n",
    "testy = corrPortal_winter.index\n",
    "testz = corrPortal_winter.values\n",
    "test = go.Figure(go.Heatmap(x=testx,y=testy,z=testz))\n",
    "plot(test, filename = 'portal winter correlation matrix.html')\n",
    "iplot(test, filename = 'portal winter correlation matrix.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winter= topcorr(corrPortal_winter,3,mydvs) \n",
    "winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popvars = ['popinYearless1','popinYearless2','popinYearless3','popinYearless4']\n",
    "weathvars = ['PRCP mean','PRCP 75%','PRCP max']\n",
    "for popvar in popvars:\n",
    "    for weathvar in weathvars:\n",
    "        r,p = stats.pearsonr(df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['winter']))]\\\n",
    "                             [popvar],df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['winter']))]\\\n",
    "                             [weathvar])\n",
    "        print('{} vs {}: r={}; p={}'.format(popvar,weathvar,r,p))\n",
    "popvars = ['popinYearless1','popinYearless2','popinYearless3','popinYearless4']\n",
    "weathvars = ['TMIN 75%']\n",
    "for popvar in popvars:\n",
    "    for weathvar in weathvars:\n",
    "        r,p = stats.pearsonr(df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['winter']))]\\\n",
    "                             [popvar],df_reg_season.loc[(df_reg_season.source=='portal')&(df_reg_season.season.isin(['winter']))]\\\n",
    "                             [weathvar])\n",
    "        print('{} vs {}: r={}; p={}'.format(popvar,weathvar,r,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other season, winter precipitation in year 0 has correlalations for population size in subsequent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Here\n",
    "\n",
    "[Back to TOC](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to model this with regression.\n",
    "two predictors: pop in year weather in year\n",
    "dv: pop in year 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model using captured juveniles to predict age class \n",
    "include weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pingouin as pg\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydvs = ['liznumberYear', 'popinYearless1', 'popinYearless2', 'popinYearless3', 'popinYearless4', 'popinYearless5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathermetrics = ['']\n",
    "iv = ['liznumberYear',weathermetric]\n",
    "dv = 'popinYearless1'\n",
    "r,p = stats.pearsonr(df_reg.liznumberYear,df_reg[var])\n",
    "print('{}: r={}; p={}'.format(var,r,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e8e978563e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TMIN mean'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinregress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliznumberYear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_reg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slope: {}    intercept: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# slope: 1.944864    intercept: 0.268578\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"R-squared: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_value\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reg' is not defined"
     ]
    }
   ],
   "source": [
    "var = 'TMIN mean' \n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_reg.liznumberYear,df_reg[var])\n",
    "print(\"slope: {}    intercept: {}\".format(slope, intercept))\n",
    "# slope: 1.944864    intercept: 0.268578\n",
    "print(\"R-squared: {}\".format(r_value**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.anova(data=df, dv='liznumberYear', between='group', detailed=True)\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
