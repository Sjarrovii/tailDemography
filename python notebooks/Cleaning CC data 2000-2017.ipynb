{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning CC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook operates on a csv created after editing in open refine and is designed to finish cleaning columns of interest which were easier to clean in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python\n",
    "\n",
    "Here we import necessary packages. \n",
    "This chunk may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'liznumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8ef8e50b5c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mliznumber\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mln\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mliztoes\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'liznumber'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import liznumber as ln\n",
    "import liztoes as lt\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_config_file(world_readable=True)\n",
    "\n",
    "# increase print limit\n",
    "pd.options.display.max_rows = 99999\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions necessary for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "def report_pattern (x , pattern , col,return_type):\n",
    "    \"\"\"searches a pandas series for a regex expression, pattern, and replaces with replacement\"\"\"\n",
    "   \n",
    "    res = print('{}:\\ntoe pattern {}:{}'.format(return_type,str(pattern),(x[col].str.match(pattern)==True).sum())) \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "#needs to capture when an entires fits multiple patterns and which patterns those are\n",
    "def label_pattern (x , pat_num , pattern, pat_col = 'toe_pattern' , col = 'toes'):\n",
    "    \"\"\"searches a pandas series for a regex expression, pattern, and replaces with replacement\"\"\"\n",
    "#     label the pattern\n",
    "    x.loc[(x[col].str.match(pattern)==True)&(x[pat_col].isnull()),pat_col] = str(pat_num)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "def make_str(x):\n",
    "    assert isinstance(x,pd.Series)\n",
    "    #convert series to string\n",
    "    x = x.astype(str)\n",
    "    #create an index of single-digit numbers\n",
    "    idx = x.str.len()<2\n",
    "    #add a zero to the beginning of those single-digit numbers\n",
    "    x.loc[idx] = '0' + x.loc[idx]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "def replace_pattern (x , pattern , pattern_b , source_col , replacement):\n",
    "    \"\"\"searches a pandas series for a regex expression, pattern, and replaces with replacement\"\"\"\n",
    "    \n",
    "    return x.loc[x[source_col].str.match(pattern)==True,source_col].str.replace(pattern_b,replacement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this chunk to read data from local folder on Chris' machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source Data\n",
    "sourceDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "sourceDataBig = 'S:/Chris/TailDemography/combined data'\n",
    "\n",
    "#Output Data paths\n",
    "outputPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "outputBig = 'S:/Chris/TailDemography/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(sourceDataBig)\n",
    "df=pd.read_csv('mapped-data-all_18-01-08_post_openrefine.csv')\n",
    "\n",
    "os.chdir(outputBig)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nThere are {} data points in our data set.\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe columns in the data have the following data types:\\n{}\".format(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting class of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to add real error handling into these conversion chunks\n",
    "\n",
    "##Convert integer columns to int\n",
    "intCols = ['meters','year']\n",
    "df[intCols]=df[intCols].astype(int,errors='ignore')\n",
    "\n",
    "##Convert numeric columns to numeric\n",
    "numCols = ['svl','tl','rtl','rtl_orig','mass']\n",
    "df[numCols]=df[numCols].apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "##Convert string columns to str\n",
    "strCols = ['toes','sex','species','vial']\n",
    "df[strCols]=df[strCols].astype(str, errors='ignore')\n",
    "\n",
    "#Convert date to datetime\n",
    "df.loc[df.date==\"NA\"]=np.nan\n",
    "df.date = pd.to_datetime(df.date,errors='coerce')\n",
    "\n",
    "##Convert bool columns to bool\n",
    "boolCols = ['review_sex','review_species','review_painted','review_new.recap',\\\n",
    "            'review_rtl','forceMale','forceFemale','forceRecap','forceNew',\\\n",
    "            'forceSighting','drop_species','drop_morphometrics','autotomized']\n",
    "df[boolCols]=df[boolCols].astype(bool, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nAfter applying the above changes, the data types are as follows:\\n{}\".format(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove leading and trailing whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for col in df:\n",
    "    print(len(col))# returns unique lengths of sex\n",
    "    col=col.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for col in df:\n",
    "    col=col.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning toes column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will rename \"toes\" to \"toes_orig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'toes':'toes_orig'},index = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new column, \"toes\"  for the renamed toes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toes'] = df.toes_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we attempt to identify problem toes name and correct or export for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pattern1 = \".( {1,}-.|.- {1,}.)\" # toes entries with any number of spaces on either side of a hyphen\n",
    "pattern2 = \".( {,}\\w{,} {1,}).\" # toes entries with space around or between numbers <- the spaces here should be deleted\n",
    "pattern3 = \".(').\"\n",
    "pattern4 = \"./.\"  # entries with '/' <-- need to replace these with '-'\n",
    "pattern5 = \"(\\?{1,})\"#<-- these needs to be investigated\n",
    "pattern6 = \"^\\d{3,}$\" # entries consist of only a single number comprised of at least three digits \n",
    "#<-- these needs to be investigated by checking raw field notes\n",
    "pattern7 = \".(-{2,}).\" # entries which have at least 2 consecutive '-' <- these should be investigated\n",
    "pattern8 = \"^0\" # entries in which single digit numbers have a leading \"0\" <-- Check raw field notes on this too\n",
    "pattern9 = \"a\\w\" #<--handled hyphens should be inserted  between the [ab] and \\w \n",
    "# entries that contain an 'a' or 'b' followed by any character in the set [a-zA-Z0-9_]\n",
    "pattern10 = \"b\\w\" #<--handled hyphens should be inserted  between the [ab] and \\w \n",
    "pattern11 = \"\\wa\" # entries that contain an 'a' or 'b' preceded by any character in the set [a-zA-Z0-9_]\n",
    "pattern12 = \"\\wb\" # entries that contain an 'a' or 'b' preceded by any character in the set [a-zA-Z0-9_]\n",
    "pattern13 = \"[()]\"\n",
    "# remove space before 'a' at end of toes\n",
    "#investigate '\\d-', \n",
    "#'-(*)-', \n",
    "#' (16) ', \n",
    "#'---', <- may not exist in raw data\n",
    "#'\\d- ', \n",
    "#'- \\d', \n",
    "#transcription errors from excel (toes in date format,\n",
    "#'-\\d\\d\\d\\d' <- may not be in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to change this block if we add or remove toe patterns.\n",
    "This is not ideal and needs to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lt.label_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_pattern = pd.Series([*range(1,14)]) \n",
    "toe_pattern = lt.make_str(toe_pattern)\n",
    "print(toe_pattern)\n",
    "\n",
    "toe_pattern_descr = pd.Series([pattern1,pattern2,pattern3,pattern4\n",
    "                               ,pattern5,pattern6,pattern7,pattern8\n",
    "                               ,pattern9,pattern10,pattern11,pattern12,pattern13])\n",
    "toe_pattern_descr = toe_pattern_descr.astype(str)\n",
    "print(toe_pattern_descr)\n",
    "\n",
    "toe_pattern_reference = pd.DataFrame({'toe_pattern': toe_pattern,'description':toe_pattern_descr})\n",
    "toe_pattern_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first replace the string 'nan' with a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.toes=='nan','toes'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many of these patterns we need to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toe_pattern'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a for-loop to label the patterns \n",
    "(there's probably a better way to do this with pandas map or apply, but I'll have to figure this out, for now this is fast enough, but it could make a difference with a larger data set or with more patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,toe_pattern_reference.shape[0]):\n",
    "    tmp_pat_num = toe_pattern_reference.iloc[i,0]\n",
    "    tmp_pattern = toe_pattern_reference.iloc[i,1]\n",
    "    df = label_pattern(df,tmp_pat_num,tmp_pattern,'toe_pattern','toes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick summary of the number of observations for each pattern in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_errors =df.toe_pattern.value_counts(dropna=False).reset_index()\\\n",
    ".rename(columns = {'index':'toe_pattern','toe_pattern':'observations'})\n",
    "toe_errors.loc[toe_errors.toe_pattern.isnull(),'toe_pattern'] = 'Not covered by current patterns'\n",
    "toe_errors_desc = toe_errors.merge(toe_pattern_reference,'left',on='toe_pattern')\n",
    "toe_errors_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we've accounted for every row in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accountedRows = toe_errors.observations.sum()\n",
    "totalRows = df.shape[0]\n",
    "notAccountedRows = df.shape[0] - toe_errors.observations.sum()\n",
    "print(\"\\nThere are {} rows accounted for in the patterns (including null values) and there {} rows in the full data set.\\\n",
    "  There are {} rows unaccounted for.\".format(accountedRows,totalRows,notAccountedRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we correct these patterns\n",
    "We'll preserve the original toe data in a column called \"toes_orig\" just in case.  We can drop this later, if we are comfortable with the changes.  The new toes will be labeled \"toes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections_config = {'01':{'action':'replace','pattern_b':\" \",'replacement':\"\\\"\\\"\"},\n",
    "            '02':{'action':'replace','pattern_b':\" \",'replacement':\"-\"},\n",
    "            '03':{'action':'replace','pattern_b':\"\\'\",'replacement':\"\\\"\\\"\"},\n",
    "            '04':{'action':'replace','pattern_b':\"/\",'replacement':\"-\"},\n",
    "            '05':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '06':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '07':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '08':{'action':'replace','pattern_b':\"^0\",'replacement':\"\\\"\\\"\"},\n",
    "            '09':{'action':'replace','pattern_b':'a','replacement':'-a'},\n",
    "            '10':{'action':'replace','pattern_b':'b','replacement':'-b'},          \n",
    "            '11':{'action':'replace','pattern_b':\"a\",'replacement':\"a-\"},\n",
    "            '12':{'action':'replace','pattern_b':\"b\",'replacement':\"b-\"},\n",
    "            '13':{'action':'replace','pattern_b':\"[()]\",'replacement':\"\\\"\\\"\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_errors_desc['action'] = toe_errors_desc.loc[toe_errors_desc.toe_pattern.str.len()==2].toe_pattern\\\n",
    ".map(lambda x: corrections_config[x]['action'],na_action='ignore')\n",
    "\n",
    "toe_errors_desc['replacement'] = toe_errors_desc.loc[toe_errors_desc.toe_pattern.str.len()==2].toe_pattern\\\n",
    ".map(lambda x: corrections_config[x]['replacement'],na_action='ignore')\n",
    "\n",
    "toe_errors_desc = toe_errors_desc.sort_values('toe_pattern').reset_index(drop=True)\n",
    "toe_errors_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,toe_errors_desc.shape[0]):\n",
    "    tmp_pat_num = toe_errors_desc.iloc[i,0]\n",
    "    tmp_pattern = toe_errors_desc.iloc[i,2]\n",
    "    action = toe_errors_desc.iloc[i,3]\n",
    "    tmp_replacement = toe_errors_desc.iloc[i,4]\n",
    "    tmp_x = df.loc[df.toe_pattern==tmp_pat_num,:]\n",
    "    \n",
    "    if action =='save':\n",
    "        tmp_filename = 'pattern'+tmp_pat_num+'.csv'\n",
    "        tmp_x.to_csv(tmp_filename)\n",
    "        print(\"Pattern {} successfully saved to {}.\".format(tmp_pattern,tmp_filename))\n",
    "    if action =='replace':\n",
    "        df.loc[df.toe_pattern==tmp_pat_num,'toes'] = replace_pattern(x=df.loc[df.toe_pattern==tmp_pat_num]\n",
    "                                                                     ,pattern = tmp_pat_num\n",
    "                                                                     ,pattern_b = tmp_pattern\n",
    "                                                                     ,source_col = 'toes'\n",
    "                                                                    ,replacement = tmp_replacement)\n",
    "        print(\"Pattern {} successfully replaced with {}.\".format(tmp_pattern,tmp_replacement))\n",
    "    else:\n",
    "        print(\"No direction provided for pattern {}.  No action was taken.\".format(tmp_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we confirm that the patterns we expect to have eliminated have indeed been eliminated from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,toe_pattern_reference.shape[0]):\n",
    "    tmp_pattern = str(toe_pattern_reference.iloc[i,1])\n",
    "    report_pattern(df,tmp_pattern,'toes','Post-Correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Sex column\n",
    "Next we move on to cleaning the \"sex\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to get an idea of the types of problems in the sex column.  We start by striping leading and trailing whitespaces.  You can see here that there were none in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sex.str.len().unique())# returns unique lengths of sex\n",
    "df.sex=df.sex.str.strip()\n",
    "print(df.sex.str.len().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify non \"m\" or \"f\" values and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patterns_sex=\"m|f|NA\"\n",
    "non_matches=df.sex.loc[df.sex.str.match(patterns_sex)!=True]\n",
    "print(\"\\nThere are {} entries for sex which do not match the patterns {}:\"\\\n",
    "      .format(non_matches.shape[0],patterns_sex.split(\"|\")))\n",
    "non_matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify values to convert to NA, m, or f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex2NA=['adult','juv','nan']\n",
    "sex2m=['unm']\n",
    "df.loc[df.sex.isin(sex2NA)==True]\n",
    "print(df.sex.loc[df.sex.isin(sex2NA)==True].count())\n",
    "print(df.sex.loc[df.sex.isin(sex2m)==True].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the values to NA or m, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.sex.isin(sex2m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.sex.isin(sex2NA),'sex']=np.nan\n",
    "df.loc[df.sex.isin(sex2m),'sex']='m'\n",
    "print(df.sex.loc[df.sex.isin(sex2NA)==True].count())\n",
    "print(df.sex.loc[df.sex.isin(sex2m)==True].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all remaining species and sex with \"?\" to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.species.str.contains('\\?')) & (df.species.notnull()),'species'] = np.nan\n",
    "df.loc[(df.sex.str.contains('\\?')) & (df.sex.notnull()),'sex'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning autotmized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotomyDict = {False:'intact',True:'autotomized'}\n",
    "\n",
    "df.loc[:,'autotomized'] = df.loc[:,'autotomized'].map(autotomyDict)\n",
    "df.autotomized.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning new.recap column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try using a dict to do thing more efficiently\n",
    "newRecapKeep = ['recap', 'new', 'r', 'n']\n",
    "new = ['new','n']\n",
    "recap = ['recap','r']\n",
    "df.loc[~df['new.recap'].isin(newRecapKeep),'new.recap'] = np.nan\n",
    "df.loc[df['new.recap'].isin(new),'new.recap'] = 'new'\n",
    "df.loc[df['new.recap'].isin(recap),'new.recap'] = 'recap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl_svl and mass_svl\n",
    "df['tl_svl']=(df.tl/df.svl)\n",
    "df['mass_svl']=(df.mass/df.svl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to generate lizardNumber \n",
    " lizard number is a numeric identifier of unique animals in the data set\n",
    "function takes the following arguments:\n",
    "    - *x*: series object on which function acts on\n",
    "    - *sortCriteria*: list of strings of column names on which to sort data.  data are sorted by columns from left to right\n",
    "    - *validationCriteria*: dictionary of dictionaries that identify columns to validate and validation expression of the form {{'column':'column_2 >= column_1'},{'otherColumn':'column_2 >= column_1'}}\n",
    "    - *result*: dictionary of of dictionaries detailing the value *x* takes if validations are True or False of the form: {{'True':x=x[i]},{'False':x=x[i]+1},{errors: 'raise'}}, errors may be 'raise' *default* (terminates function and returns an error) or 'ignore' (returns 'NA')\n",
    "Function action:\n",
    "- first sort data by species, toes, then date\n",
    "\n",
    "- for time points 1 and , with 2 being later: \n",
    "    - toes2 == toes1\n",
    "    - svl2-svl1 >=-2\n",
    "    - year2-year1 <=7\n",
    "    - for species ==j:\n",
    "        - if svl >=56:\n",
    "            - if sex2==sex1:\n",
    "                lizardNumber[i+1]=lizardNumber[i]\n",
    "          else:\n",
    "            - lizardNumber[i+1]=lizardNumber[i]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sortCriteria = ['species','toes', 'sex']\n",
    "validation = ['date','svl']\n",
    "\n",
    "\n",
    "def lizsort(x,path:str,sortCriteria = ['species','toes', 'sex'], validation = ['date','svl'],\\\n",
    "            unsortablefile ='unsortable.csv'):\n",
    "    \"\"\"\n",
    "    takes a pandas data frame and returns a pandas dataframe with only those values which \n",
    "    can be evaluated according to given criteria and prints a summaryof the files evaluated\n",
    "    :param path:\n",
    "    :param sortCriteria:\n",
    "    :param validation:\n",
    "    :param unsortablefile:\n",
    "    \"\"\"\n",
    "    #identify lizards with sufficient data to evaluate\n",
    "    #report on those without sufficient data and save them to a file for later evaluation\n",
    "    critical = sortCriteria +validation\n",
    "    unsortable = x.loc[x.loc[:,critical].isnull().any(axis=1)]\n",
    "    sortable = x.loc[x.loc[:,critical].notnull().all(axis=1)]\n",
    "    os.chdir(path)\n",
    "    unsortable.to_csv(unsortablefile)\n",
    "    print(\"\\nThere were {} entries for which values for one of the critical criteria, ({}), were null.  \\\n",
    "    These entries could not be evaluated and were written out to the file {} for evaluation.\"\\\n",
    "          .format(unsortable.shape[0],critical,unsortablefile))\n",
    "    return sortable\n",
    "\n",
    "def mindate(x, sortCriteria = ['species','toes', 'sex']): # finds date of the initial capture of an animal\n",
    "    \"\"\"\n",
    "    takes a pandas data frame and returns a dataframe with sorting criteria adds a column containing the earliest date \n",
    "    at which each unique combination of the sort criteria was sighted. [Requires that the source dataframe,x, has a \n",
    "    column labeled 'date'.]\n",
    "    \"\"\"\n",
    "    if any(x.columns=='initialCaptureDate'):\n",
    "        x = tmp_sort['n_val_data'].drop('initialCaptureDate',1)\n",
    "    sortable_min_date =pd.DataFrame(x.groupby(sortCriteria).date.min()).\\\n",
    "    rename(index = str, columns= {'date':'initialCaptureDate'}).reset_index()\n",
    "    x = x.merge(sortable_min_date,how = 'left', on = sortCriteria)\n",
    "    x['year_diff'] = x.date.dt.year - x.initialCaptureDate.dt.year\n",
    "    return x\n",
    "\n",
    "def smallest(x, svlGroup = ['species','toes', 'sex','initialCaptureDate']):#finds svl of animal at date of the initial capture.  needs to be moved out of function\n",
    "    if any(x.columns=='smallest_svl'):\n",
    "        x = x.drop('smallest_svl',1)\n",
    "    sortable_smallest_svl =x.groupby(svlGroup).svl.min().reset_index()\\\n",
    "    .rename(index = str, columns= {'svl':'smallest_svl'})\n",
    "    #sortable_smallest_svl\n",
    "    x = x.merge(sortable_smallest_svl,how = 'left', on = svlGroup)\n",
    "    x['svl_diff'] = x.svl - x.smallest_svl\n",
    "    return x\n",
    "\n",
    "def validate (x, sortCriteria = ['species','toes', 'sex'],validation = ['date','svl']):\n",
    "    x['tmp'] = 1 \n",
    "    numbers = x.loc[(x.year_diff<=7) & (x.svl_diff>=-2),:].\\\n",
    "    groupby(['species','sex','toes']).tmp.min().cumsum().reset_index()\n",
    "    validated = x.loc[(x.year_diff<=7) & (x.svl_diff>=-2),:].shape[0]\n",
    "    not_val_data = x.loc[(x.year_diff<=7) & (x.svl_diff>=-2),:]\n",
    "    not_validated = x.loc[~((x.year_diff<=7) & (x.svl_diff>=-2)),:].shape[0]\n",
    "    numbers = numbers.rename(columns={'tmp':'liznumber'}) # rename last column to liznumber\n",
    "    #the next line merges the numbers to the original data frame to assign the lizard number to the full record\n",
    "    #of an animal.  It then drop 'tmp'and 'smallest_svl, sinc ewe won't be using these again\n",
    "    x = x.merge(numbers,'left', on = ['species','sex','toes']).drop(['tmp','smallest_svl'],1)\n",
    "    print(\"\\nOf those entries we can handle, there are {} individuals as defined by {} which pass validataion based\\\n",
    "    on {} and {} which do not pass validation.\"\\\n",
    "          .format(validated,sortCriteria,validation,not_validated))\n",
    "    return {'val_data':x,'n_val_data':not_val_data,'n_validated':not_validated}\n",
    "\n",
    "# def genliznum2(df, path:str, errors:str= 'raise'):\n",
    "#     \"\"\"\n",
    "#     calls functions to generate a unique identifier for each lizard\n",
    "    \n",
    "#     Lizard number is a numeric identifier of unique animals in the data set function takes the following arguments:\n",
    "#     :param df:  series object on which function acts on\n",
    "#     :param sortCriteria: list of strings of column names on which to sort data.  data are sorted by columns from left \\\n",
    "#     to right\n",
    "#     :param validation: dictionary of dictionaries that identify columns to validate and validation expression of the form:\\\n",
    "#      {{'column':'column_2 >= column_1'},{'otherColumn':'column_2 >= column_1'}}\n",
    "#     :param errors: str , errors may be 'raise' *default* (terminates function and returns an error) or 'ignore' (returns 'NA')\n",
    "#     :return: dataframe\n",
    "#     #dictionary  of dictionaries detailing the value *x* takes if validations are True or False of the form: \\\n",
    "#     #{{'True':x=x[i]},{'False':x=x[i]+1},{errors: 'raise'}}\n",
    "#     \"\"\"\n",
    "#     sortable = lizsort(df, path = path)\n",
    "#     sortable = mindate(sortable)\n",
    "#     sortable = smallest(sortable)\n",
    "#     tmp_sort = validate(sortable)\n",
    "#     sortable = tmp_sort['val_data']\n",
    "#     n_val = mindate(tmp_sort['n_val_data'])\n",
    "#     n_val = smallest(n_val)\n",
    "#     n_val = validate(n_val)['val_data']\n",
    " \n",
    "#     res = n_val\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genliznum2(df, path = 'C:\\\\Users\\\\Christopher\\\\Documents\\\\GitHub\\\\tailDemography\\\\data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial attempt to assign lizard numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortable = lizsort(df, path = 'S:\\\\Chris\\\\TailDemography\\\\data')\n",
    "    \n",
    "sortable = mindate(sortable)\n",
    "sortable = smallest(sortable)\n",
    "tmp_sort = validate(sortable)\n",
    "sortable = tmp_sort['val_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt to assign lizard numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = mindate(tmp_sort['n_val_data'])\n",
    "n_val = smallest(n_val)\n",
    "df_numbered = validate(n_val)['val_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the output data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC of lizard numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify individuals that have same species and toes, but different sex for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df.groupby(['species','toes']).sex.nunique().reset_index().rename(columns = {'sex':'sex_count'})\\\n",
    "         ,how = 'inner', on = ['species','toes'])\n",
    "print(df.loc[df.sex_count>1,:].shape[0])\n",
    "df.loc[df.sex_count>1,:].to_csv('entries flagged with same species and toes diff sex.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['species','toes']).sex.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lizard Numbers in the sample range from {} to {}.\"\\\n",
    "      .format(df_numbered.liznumber.min(),df_numbered.liznumber.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possibleLizNum = set(range(int(df_numbered.liznumber.min()),int(df_numbered.liznumber.max())))\n",
    "actualLizNum = set(pd.Series(df_numbered.liznumber.unique()).dropna().apply(int))\n",
    "print(\"\\nThere are {} entries.  There are {} unique lizard numbers.\\\n",
    "\\n\\nThe liznumber ranges from {} to {}.\"\\\n",
    "  .format(df_numbered.shape[0],len(df_numbered.liznumber.unique())\\\n",
    "          ,df_numbered.liznumber.min(),df_numbered.liznumber.max()))\n",
    "\n",
    "missingLizNum = possibleLizNum - actualLizNum\n",
    "if len(missingLizNum)>0:\n",
    "    print(\"\\n\\nThe following numbers are not assigned to a lizard:\\n{}\"\\\n",
    "      .format(missingLizNum))\n",
    "else:\n",
    "    print(\"\\n\\nThere are no numbers which were not assigned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional columns\n",
    "- *daysSinceCapture* [int]:identifies the number of days since the animal was captured\n",
    "- *capture* [int]: identifies the number of times an animal has been captured prior to an entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered.loc[:,'daysSinceCapture'] = (df_numbered.date - df_numbered.initialCaptureDate).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to QC this\n",
    "df_numbered['capture'] = df_numbered.sort_values(['liznumber','date'])\\\n",
    ".groupby(['liznumber']).daysSinceCapture.cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_numbered.loc[df_numbered.species.isin(['j','v'])].groupby('capture').capture.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Histogram(x = df_numbered.groupby('liznumber').capture.max())]\n",
    "py.iplot(data, filename = 'Frequency of Captures in Crystal Creek 2000 - 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lizards = [go.Scatter(x = df_numbered.liznumber,\n",
    "                   y = df_numbered.groupby('liznumber').daysSinceCapture.max(), \n",
    "                     mode = 'markers')]\n",
    "# year1 = [go.scatter.Line(y = 365)]\n",
    "# year2 = [go.scatter.Line(y = 365*2)]\n",
    "# year3 = [go.scatter.Line(y = 365*3)]\n",
    "# year4 = [go.scatter.Line(y = 365*4)]\n",
    "# year5 = [go.scatter.Line(y = 365*5)]\n",
    "# year6 = [go.scatter.Line(y = 365*6)]\n",
    "# year7 = [go.scatter.Line(y = 365*7)]\n",
    "# year8 = [go.scatter.Line(y = 365*8)]\n",
    "\n",
    "# data = [lizards, year1, year2, year3, year4, year5, year6, year7, year8]\n",
    "data = lizards\n",
    "layout = go.Layout(\n",
    "    title = 'Days Since Initial Capture in Crystal Creek 2000 - 2017',\n",
    "        titlefont = dict(\n",
    "            size = 20),\n",
    "    xaxis = dict(\n",
    "            title='Lizard Number',\n",
    "            titlefont=dict(\n",
    "                size=18)),\n",
    "    yaxis = dict(\n",
    "            title='Greatest Number of Days Since<br> Initial Capture',\n",
    "            titlefont=dict(\n",
    "                size=18)))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename = 'Days Since Initial Capture in Crystal Creek 2000 - 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfF = df_numbered.loc[df_numbered.sex =='f']\n",
    "dfM = df_numbered.loc[df_numbered.sex =='m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "females = go.Scatter(\n",
    "    x = dfF.liznumber,\n",
    "    y = dfF.groupby('liznumber').daysSinceCapture.max(),\n",
    "    name = 'females',\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = 'rgba(152, 0, 0, .8)',\n",
    "        opacity = 0.75,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "            color = 'rgb(0, 0, 0)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "males = go.Scatter(\n",
    "    x = dfM.liznumber,\n",
    "    y = dfM.groupby('liznumber').daysSinceCapture.max(),\n",
    "    name = 'males',\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = 'rgba(255, 182, 193, .9)',\n",
    "        opacity = 0.75,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [females, males]\n",
    "\n",
    "layout = dict(title = 'Days Since Initial Capture in Crystal Creek 2000 - 2017 By Sex',\n",
    "              yaxis = dict(\n",
    "                  title='Greatest Number of Days Since<br> Initial Capture',\n",
    "                  titlefont=dict(\n",
    "                      size=18)\n",
    "              ),\n",
    "              xaxis = dict(zeroline = False)\n",
    "             )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Days Since Initial Capture in Crystal Creek 2000 - 2017 By Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males = go.Histogram(x = df_numbered.loc[df.sex == 'm','capture'],opacity= 0.75,name='males')\n",
    "females = go.Histogram(x = df_numbered.loc[df.sex == 'f','capture'], opacity= 0.75, name = 'females')\n",
    "data = [males,females]\n",
    "py.iplot(data, filename = 'Frequency of Captures by Sex in Crystal Creek 2000 - 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC of Capture number and Recap status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recapQuestion=df_numbered.loc[(df_numbered.capture==1 )& (df_numbered[\"new.recap\"]=='recap'),:]\n",
    "print(\"There are {} instances in rows for which a lizard appears to have only one capture, but is listed as a recap.\\\n",
    "The distribution of these across years in the sample is as follows:\\n{}.\"\\\n",
    "      .format(recapQuestion.shape[0],recapQuestion.year.value_counts()))\n",
    "recapQuestion.to_csv(\"Questionable recaptures.csv\")#These individuals need to be rechecked in the raw notes\n",
    "recapQuestion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recapQuestion.loc[recapQuestion.svl<54,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we export the cleaned data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered = df_numbered.rename(index = str, columns = {'new.recap':'newRecap'})\n",
    "qc_drop_cols = df_numbered.columns[df_numbered.columns.str.contains('force|drop')]\n",
    "df_full = df_numbered.drop(qc_drop_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = (pd.to_datetime('now')-pd.Timedelta(hours=4))\n",
    "timestamp = str(timestamp).replace(':','_')\n",
    "#path=''C:\\\\Users\\\\Christopher\\\\Google Drive\\\\TailDemography\\\\outputFiles\\\\''\n",
    "# path=outputBig\n",
    "filename = 'cleaned CC data 2000-2017_' + timestamp+ '.csv'\n",
    "# filename = path + '/cleaned CC data 2000-2017' + '.csv'\n",
    "df_full.to_csv(filename,index = False)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
