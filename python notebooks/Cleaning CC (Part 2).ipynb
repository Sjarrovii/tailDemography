{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning CC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook operates on a csv created after editing in open refine and is designed to finish cleaning columns of interest which were easier to clean in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Resume Here](#Resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Outstanding Problems](#Outstanding-Problems)\n",
    "\n",
    "1. [Setting up Python](#Setting-up-Python)\n",
    "    \n",
    "    1. [Setting the Location](#Setting-the-Location)\n",
    "    \n",
    "    2. [Importing Data](#Importing-Data)\n",
    "    \n",
    "    3. [Preparing for a Save](#Preparing-for-a-Save)\n",
    "    \n",
    "4. [Functions](#Functions)\n",
    "    1. [appendstr](#appenstr)\n",
    "    2. [typeordrop](#typeordrop)\n",
    "    3. [myint](#myint)\n",
    "    3. [testint](#testint)\n",
    "    4. [rom2arab](#rom2arab)\n",
    "    5. [exportliz](#exportliz)\n",
    "    \n",
    "2. [Inspecting the Data](#Inspecting-the-Data)\n",
    "3. [Cleaning Data](#CleaningData)\n",
    "    1. [Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "        1. [rtl](#rtl)\n",
    "        2. [tl](#tl)\n",
    "        3. [svl](#svl)\n",
    "        4. [autotomized](#autotomized)\n",
    "        5. [toes](#toes)\n",
    "        6. [sex](#sex)\n",
    "        7. [species](#species)\n",
    "        7. [new.recap](#new.recap)\n",
    "        8. [date](#date)\n",
    "    2. [Correcting Class of Columns](#Correcting-Class-of-Columns)\n",
    "    \n",
    "4. [Adding New Columns](#Adding-New-Columns)\n",
    "\n",
    "    1. [TL_SVL](#TL_SVL)\n",
    "    \n",
    "    2. [Mass_SVL](#Mass_SVL)\n",
    "    \n",
    "    3. [Lizard Number](#Lizard-Number)\n",
    "         - [Assign Lizard Numbers](#Assign-Lizard-Numbers)\n",
    "         - [QC the Numbers](#QC-the-Numbers)\n",
    "    \n",
    "    4. [Days Since Capture](#Days-Since-Capture)\n",
    "\n",
    "    5. [Capture Number](#Capture-Number)\n",
    "\n",
    "5. [Export Cleaned Data](#Export-Cleaned-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outstanding Problems\n",
    "\n",
    "1. [outstanding1](#outstanding1)\n",
    "2. [outstanding2](#outstanding2)\n",
    "3. [outstanding3](#outstanding3)\n",
    "4. [outstanding4](#outstanding4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Python\n",
    "\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "Here we import necessary packages. \n",
    "This chunk may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob,logging\n",
    "from liz_number import lizsort,mindate,smallest,validate\n",
    "from liz_toes import make_str,label_pattern,replace_pattern,report_pattern\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_config_file(world_readable=True)\n",
    "logging.basicConfig(filename='S:\\\\Chris\\\\TailDemography\\\\TailDemography\\\\Scripts and notes\\\\Cleaning CC (Part 2).log'\n",
    "                    , filemode='a',\n",
    "                    format='%(funcName)s - %(levelname)s - %(message)s - %(asctime)s', level=logging.INFO)\n",
    "# increase print limit\n",
    "pd.options.display.max_rows = 99999\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Location\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "These chunks identify the locations from which we can get data and to which we can save data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data\n",
    "Source files can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/Cleaned Combined Data'\n",
    "sourceDataBig = 'S:/Chris/TailDemography/TailDemography/Cleaned Combined Data'\n",
    "# sourceBlack = 'C:/Users/test/Desktop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate Source Data\n",
    "Intermediate files can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceInterDataPers = 'C:/Users/Christopher/Google Drive/TailDemography/Intermediate Files/DeepCleaning'\n",
    "sourceinterDataBig = 'S:/Chris/TailDemography/TailDemography/Intermediate Files/DeepCleaning'\n",
    "# sourceBlack = 'C:/Users/test/Desktop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data\n",
    "Outputfiles can be found in the following locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPers = 'C:/Users/Christopher/Google Drive/TailDemography/outputFiles'\n",
    "outputBig = 'S:/Chris/TailDemography/TailDemography/outputFiles'\n",
    "# outputBlack = 'C:/Users/test/Desktop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImportingData'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "Here we import data from one of the available locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Appended and Trimmed CC Data 2000-2017_2019-03-10 13hrs38min.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(sourceDataBig)\n",
    "files = glob.glob('*.csv')\n",
    "latest = files[-1]\n",
    "latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>toes</th>\n",
       "      <th>sex</th>\n",
       "      <th>date</th>\n",
       "      <th>svl</th>\n",
       "      <th>tl</th>\n",
       "      <th>rtl</th>\n",
       "      <th>autotomized</th>\n",
       "      <th>mass</th>\n",
       "      <th>location</th>\n",
       "      <th>meters</th>\n",
       "      <th>new.recap</th>\n",
       "      <th>painted</th>\n",
       "      <th>sighting</th>\n",
       "      <th>paint.mark</th>\n",
       "      <th>vial</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>3-7-11-19</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-07-14 00:00:00</td>\n",
       "      <td>63</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>halfway up to site</td>\n",
       "      <td>-200</td>\n",
       "      <td>NEW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b101t</td>\n",
       "      <td>toes in vial 58-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>3-7-11-18</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-07-14 00:00:00</td>\n",
       "      <td>66</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8</td>\n",
       "      <td>left downstream 100m v 1 falls</td>\n",
       "      <td>-100</td>\n",
       "      <td>NEW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b102t</td>\n",
       "      <td>toes in vial 59-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>3-7-12-16</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-07-14 00:00:00</td>\n",
       "      <td>68</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3</td>\n",
       "      <td>90m v 1 falls</td>\n",
       "      <td>-90</td>\n",
       "      <td>NEW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b103t</td>\n",
       "      <td>toes in vial 60-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-07-05 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sb at intersection with trail v 1 falls</td>\n",
       "      <td>-30</td>\n",
       "      <td>sighting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-07-14 00:00:00</td>\n",
       "      <td>85</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.5</td>\n",
       "      <td>sb - trail intersection v 1 falls</td>\n",
       "      <td>-20</td>\n",
       "      <td>recap</td>\n",
       "      <td>toe loss may be natural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b104t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species        toes  sex                 date  svl   tl  rtl  autotomized  \\\n",
       "0      sj   3-7-11-19    m  2002-07-14 00:00:00   63   92    0          NaN   \n",
       "1      sj   3-7-11-18    m  2002-07-14 00:00:00   66   92    0          NaN   \n",
       "2      sj   3-7-12-16    m  2002-07-14 00:00:00   68  103    0          NaN   \n",
       "3      sv         NaN  NaN  2002-07-05 00:00:00  NaN  NaN  NaN          NaN   \n",
       "4      sj          10    m  2002-07-14 00:00:00   85  118    0          NaN   \n",
       "\n",
       "   mass                                 location meters new.recap  \\\n",
       "0    10                       halfway up to site   -200       NEW   \n",
       "1  10.8           left downstream 100m v 1 falls   -100       NEW   \n",
       "2  10.3                            90m v 1 falls    -90       NEW   \n",
       "3   NaN  sb at intersection with trail v 1 falls    -30  sighting   \n",
       "4  19.5        sb - trail intersection v 1 falls    -20     recap   \n",
       "\n",
       "                   painted  sighting paint.mark                vial misc  \n",
       "0                      NaN       NaN      b101t  toes in vial 58-02  NaN  \n",
       "1                      NaN       NaN      b102t  toes in vial 59-02  NaN  \n",
       "2                      NaN       NaN      b103t  toes in vial 60-02  NaN  \n",
       "3                      NaN       NaN        w2a                 NaN  NaN  \n",
       "4  toe loss may be natural       NaN      b104t                 NaN  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(latest)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for a Save\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "Now we change the working directory so that inermediate files are saved to our preferred location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(sourceinterDataBig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "In this section we find functions written for this notebook.  We will need to consider whether or not to add these to the code library.\n",
    "\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "1. [appendstr](#appenstr)\n",
    "2. [typeordrop](#typeordrop)\n",
    "3. [myint](#myint)\n",
    "3. [testint](#testint)\n",
    "4. [rom2arab](#rom2arab)\n",
    "5. [exportliz](#exportliz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appendstr\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendstr(x, value, connector = '', position=0):\n",
    "    \"\"\"\n",
    "    appends *value* and *x* separated by a *connector* with the position of *val* determined by *position*\n",
    "    :param x:\n",
    "    :param value:\n",
    "    :param connector:\n",
    "    :param position:\n",
    "    \"\"\"\n",
    "    assert((isinstance(x,str)|(x is None)|(x!=x))),\"x must be str type, NoneType or NaN: x is {} type.\"\\\n",
    "    .format(type(x))\n",
    "    assert(isinstance(value,str)),\"value must be str type: value is {} type.\".format(type(value))\n",
    "    assert(isinstance(connector,str))\\\n",
    "    , \"connector must be str or None type, not {} type.\".format(type(connector))\n",
    "    assert(isinstance(position,(int))), \"position must be int type, not {}.\"\\\n",
    "           .format(type(position)) \n",
    "    if isinstance(position,int):\n",
    "        if ((x!=x)|(x is None)|(x =='')):\n",
    "            x=''\n",
    "            position=0\n",
    "#             assert(position == 0),\"If x is NaN or len(x)==0, position must be 0 not {}.\".format(position)\n",
    "        else:\n",
    "            x = x\n",
    "        assert(position in [0]+[i for i in range(-1,len(x)-1)])\\\n",
    "        , \"position must be a value in the range -1 through {}.\".format(len(x)-1)\n",
    "    try:\n",
    "        prefix = x[:position]\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Exception occurred in generating prefix.\")\n",
    "    try:\n",
    "        suffix = x[position:]\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Exception occurred in generating suffix.\")\n",
    "        \n",
    "    if len(x)==0:\n",
    "        res = value\n",
    "    res = prefix+connector+value+connector+suffix\n",
    "\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how *appendstr* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b_test_ar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo='bar'\n",
    "appendstr(foo,'test',connector='_',position=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_test_bar'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo='bar'\n",
    "appendstr(foo,'test',connector='_',position=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appendstr('','test',position=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## typeordrop\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeordrop(x,typ,replace=None, verbose=True):\n",
    "    \"\"\"this function attempts to force an object, *x*, to a particular type,*typ*. If this is not possible, \n",
    "    it reports the value of the object that could not be forced and replaces the object with the value \n",
    "    supplied to the *replace* argument\"\"\"\n",
    "    if not isinstance(x,typ)==True:\n",
    "        while False:\n",
    "            try:\n",
    "                x=typ(x)\n",
    "                logging.info(\"Working as expected\")\n",
    "                break\n",
    "            except Excetion as e:\n",
    "                logging.exception(\"Could not force value supplied to 'x' argument to f'{typ} type. x is f'{x} type.\")\n",
    "                if verbose==True:\n",
    "                    print(\"Could not force value supplied to 'x' argument to {} type. x is {} type:\\n\\n x = {}\"\\\n",
    "                          .format(typ,type(x),x))\n",
    "                x = replace\n",
    "    else:\n",
    "        logging.info(\"f'{x} is already of type f{'typ'}.\")\n",
    "    return x\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of how *typeordrop* works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', 'bar']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=['foo','bar']\n",
    "typeordrop(x,int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## myint\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myint(x, verbose = False):\n",
    "    try:\n",
    "        x = str(x).split('.')[0]\n",
    "    except Exception as e:\n",
    "        typ = type(x)\n",
    "        logging.exception(\"f'{x} is of type f'{typ} and cannot be forced to int.\")\n",
    "        x = x\n",
    "        if verbose == True:\n",
    "            print('{} is of type {} and cannot be forced to int.'.format(x,type(x)))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is are a few examples of how [*myint*](#myint) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'NoneType'>, <class 'float'>, <class 'str'>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['None', '1', 'f']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar = [None, 1.0, \"f\"]\n",
    "print([type(x) for x in bar])\n",
    "[myint(x) for x in bar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'NoneType'>, <class 'float'>, <class 'str'>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['None', '2001', '2001']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar = [None, 2001.0, \"2001.0\"]\n",
    "print([type(x) for x in bar])\n",
    "[myint(x,True) for x in bar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testint\n",
    "\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testint(x):\n",
    "    try:\n",
    "        res = int(x)\n",
    "    except Exception as e:\n",
    "        logging.error(\"f'{x} could not be forced to int.\")\n",
    "        res = x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ix2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2ii2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10vii2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30viii2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30-9-2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date\n",
       "0     9ix2010\n",
       "1     2ii2010\n",
       "2   10vii2011\n",
       "3  30viii2009\n",
       "4   30-9-2011"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = ['9ix2010', '2ii2010', '10vii2011','30viii2009','30-9-2011']\n",
    "foo = pd.DataFrame(foo).rename(columns={0:'date'})\n",
    "dict_rom2arab = {'i':'1','ii':'2','iii':'3','iv':'4','v':'5','vi':'6','vii':'7','viii':'8','ix':'9','x':'10'\n",
    "        ,'xi':'11','xii':'12'}\n",
    "print(dict_rom2arab.keys())\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testint(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testint('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, True, True, True, True]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[isinstance(testint(char),int) for char in foo.date[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False, True, False, True, True, True, True]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[isinstance(testint(char),int) for char in foo.date[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rom2arab\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rom2arab(x,new_dict= None, verbose = False, replace = False):\n",
    "    \"\"\"checks keys in new_dict to determine which keys the string portion of, x, and replaces the at string with \n",
    "    the corresponding value in new_dict.\"\"\"\n",
    "    dict_rom2arab = {'i':'01','ii':'02','iii':'03','iv':'04','v':'05','vi':'06','vii':'07','viii':'08'\n",
    "                     ,'ix':'09','x':'10','xi':'11','xii':'12'}\n",
    "    if new_dict is None:\n",
    "        new_dict = dict_rom2arab \n",
    "    idx_str = [isinstance(testint(char),int) for char in x]\n",
    "    idx_tmp = range(0,len(x))\n",
    "    str_df = pd.DataFrame({'numeric':idx_str,'index':idx_tmp})\n",
    "    idx_strings = str_df.loc[str_df.numeric==False,'index']\n",
    "    idx_numeric = str_df.loc[str_df.numeric==True,'index']\n",
    "    val2replace = x[idx_strings.min():idx_strings.max()+1] # This assumes that all string values are contiuguos.\n",
    "    try:\n",
    "        newval = '-'+new_dict[val2replace]+'-'\n",
    "        if verbose is True:\n",
    "            res =[\"replaced value: \\'{}\\'\".format(val2replace),x.replace(val2replace,newval)]\n",
    "        else:\n",
    "            res = x.replace(val2replace,newval)\n",
    "    except:\n",
    "        res = x\n",
    "        if verbose is True:\n",
    "            print(\"x has no string values contained in 'new_dict'. \\nx:{}\".format(x))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom2arab(foo.date[0],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom2arab(foo.date[0],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom2arab(foo.date[4],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo.date.apply(rom2arab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exportliz\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "[Back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportliz(df,iterator, lizard = None, prefix = None, verbose = False):\n",
    "    \"\"\"creates a filename for each lizards and then saves that lizards data to that filename. \n",
    "    Can take a list of lizards or iterate over the entire dataframe\"\"\"\n",
    "    assert isinstance(df,pd.DataFrame), \"df must be pandas DataFrame, not {}.\".format(type(df))\n",
    "    assert ((lizard is None) |(isinstance(lizard,list))), \"lizard must be Nonetype or list, not {}.\".format(lizard)\n",
    "    assert ((prefix is None) |(isinstance(prefix,str))), \"lizard must be Nonetype or str, not {}.\".format(lizard)\n",
    "    assert iterator in df.columns, \"iterator must be in df.columns:\\n {}\".format(df.columns)\n",
    "    if prefix is None:\n",
    "        prefix = \"File for lizard \" \n",
    "    suffix = \".csv\"\n",
    "    if lizard is not None:\n",
    "        assert lizard in df[iterator].unique(), \"lizard must be None or contained in df[iterator].\"\n",
    "        for liz in lizard:\n",
    "            filename = prefix + str(liz) + suffix\n",
    "            if verbose == True:\n",
    "                print(\"liz type:{}\\nliz:{}\\ndf[iterator]:{}\".format(type(liz),liz,type(df[iterator])))\n",
    "            data = df.loc[df[iterator] == liz,:]\n",
    "            data.to_csv(filename)\n",
    "    else:\n",
    "        for liz in pd.unique(df[iterator]):\n",
    "            filename = prefix + str(liz) + suffix\n",
    "            if verbose == True:\n",
    "                print(\"liz:{}\\nliz:{}\\ndf[iterator]:{}\".format(type(liz),liz,type(df[iterator])))\n",
    "            data = df.loc[df[iterator] == liz,:]\n",
    "            data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to Functions](#Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Data\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "Let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nThere are {} data points in our data set.\".format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nThe columns in the data have the following data types:\\n{}\".format(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'CleaningData'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "Now we get to the actual cleaning of the data.  We will inspect the data and take the appropriate cleaning steps:\n",
    "1. [Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "2. [Correcting Class of Columns](#Correcting-Class-of-Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column-by-Column Cleaning\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "We will handle the cleaning for each column in this section.\n",
    "1. [rtl](#rtl)\n",
    "2. [tl](#tl)\n",
    "3. [svl](#svl)\n",
    "4. [autotomized](#autotomized)\n",
    "    1. [creating 'rtl_orig'and relabeling 'rtl' and 'autotomized](#creating-'rtl_orig'-and-relabeling-'rtl'-and-'autotomized')\n",
    "        - [copy the values in rtl to a new column, *rtl_orig*](#copy-the-values-in-rtl-to-a-new-column)\n",
    "        - [relabel entries in the autotomized column based on the values in the rtl_orig column](#relabel-entries-in-the-autotomized-column-based-on-the-values-in-the-rtl_orig-column) \n",
    "        - [relabel entries in the rtl column](#relabel-entries-in-the-rtl-column)\n",
    "5. [toes](#toes)\n",
    "6. [sex](#sex)\n",
    "7. [species](#species)\n",
    "8. [new.recap](#new.recap)\n",
    "9. [date](#date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rtl\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "Here we investigate and clean values in the column 'rtl'. These should be int type values that are greater than or equal to -1.  First, we test to see if all of the values are of type int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "badtypes = []\n",
    "for val in df.rtl:\n",
    "    try:\n",
    "        x = isinstance(type(int(val)),int)\n",
    "    except:\n",
    "        badtypes=badtypes+[val]\n",
    "print(\"'badtypes' represents {} entries in the df:\".format(len(badtypes)))\n",
    "if len(badtypes)==0:\n",
    "    print(\"\\nAll values in df.rtl can be successfuly converted to int.\\n\\n\")\n",
    "#     df['rtl'] = df.rtl.apply(int)\n",
    "else:\n",
    "    print(\"\\nAll values in df.rtl could not be converted to int.  The following values could not be \\\n",
    "converted and should be investigated:\\n\\n{}\\n\\nbadtypes values are distributed as follows in the df:\\n\\n{}\"\\\n",
    "          .format(list(set(badtypes)),df.loc[df.rtl.isin(badtypes),'rtl'].value_counts(dropna=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-NaN values are few, so we will inspect these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',100000)\n",
    "df.loc[(df.rtl.isin(badtypes))&(df.rtl.notna()),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on review discussions, we will make the changes below:\n",
    "- ‘?’--> 0; misc: “unsure if tail was recently broken at very tip”\n",
    "- ‘o’--> 0\n",
    "- ‘32 -12’ -->32; misc: “potential double-break at 12 \\[george to check before use\\]” \n",
    "- ‘-’--> NaN\n",
    "- ‘10(kink)’-->0; misc:”kink at 10mm”\n",
    "We will use the function [*appendstr*](#appendstr) to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"‘?’--> 0; misc: “unsure if tail was recently broken at very tip”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ques = (df.rtl.isin(badtypes))&(df.rtl=='?')\n",
    "df[idx_ques]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_ques,'misc']= df.loc[idx_ques,:].misc\\\n",
    ".apply(lambda x: appendstr(x,\"unsure if tail was recently broken at very tip\",';'))\n",
    "df.loc[idx_ques,'rtl']= '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entries now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_ques,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"‘o’--> 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_o = (df.rtl.isin(badtypes))&(df.rtl=='o')\n",
    "df[idx_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_o,'rtl']= '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entries now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_o,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"‘32-12’ -->32; misc: “potential double-break at 12 \\[george to check before use\\]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_32 = (df.rtl.isin(badtypes))&(df.rtl=='32 -12')\n",
    "df.loc[idx_32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_32,'misc']= df.loc[idx_32,:].misc\\\n",
    ".apply(lambda x: appendstr(x,\"potential double-break at 12 [george to check before use]\",';'))\n",
    "\n",
    "df.loc[idx_32,'rtl']= '32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entries now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_32,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"‘-’-->'NaN'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_minus = (df.rtl.isin(badtypes))&(df.rtl=='-')\n",
    "df.loc[idx_minus,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also address the values for svl and tl in this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_minus,['rtl','tl','svl']]= np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entries now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_minus,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘10(kink)’-->0; misc:”kink at 10mm” We will use the function appendstr to do this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_10kink = (df.rtl.isin(badtypes))&(df.rtl=='10(kink)')\n",
    "df.loc[idx_10kink,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_10kink,'misc']= df.loc[idx_10kink,:].misc.apply(lambda x: appendstr(x,\"kink at 10mm\",';'))\n",
    "df.loc[idx_10kink,'rtl']= '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These entries now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_10kink,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will inspect those that had at least one other length measurement (svl or tl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('max_colwidth')\n",
    "idx_rtlnaplus1 = (df.rtl.isna())&(((df.svl.isna())&~(df.tl.isna()))|(~(df.svl.isna())&(df.tl.isna())))\n",
    "df.loc[idx_rtlnaplus1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but one of these was a sighting.  We will have to look at the field notes to confirm whether or not data were actually missing for the remaining entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[(df.rtl.isna())&((df.svl.notna())|(df.tl.notna()))&df['new.recap'].str.contains('recap'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have addressed these, we will force rtl to an int type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check to see for out of range rtl values, *i.e.* rtl values less than -1 or suspiciously high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude 0 and -1 values for rtl in these figures because of the large proportion of in range values they account for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfnobadtypes0neg1 = (~df.rtl.isin(badtypes))&(~df.rtl.isin(['0','-1']))\n",
    "dfother = ~(df.species.dropna().str.contains('v|j'))&(df.species.notna())&(dfnobadtypes0neg1)\n",
    "jarrovii = go.Histogram(x = df.loc[(df.species.str.contains('j'))&(dfnobadtypes0neg1)\n",
    "                                   ,'rtl'].astype(int, 'ignore'),name = 'S. jarrovii',xbins =dict(size=1)\n",
    "                        #,histnorm='probability'\n",
    "                        , cumulative=dict(enabled = False, direction = 'increasing'))\n",
    "virgatus = go.Histogram(x = df.loc[(df.species.str.contains('v'))&(dfnobadtypes0neg1)\n",
    "                                   ,'rtl'].astype(int, 'ignore'), name = 'S. virgatus',xbins =dict(size=1)\n",
    "                       #,histnorm='probability'\n",
    "                        , cumulative=dict(enabled = False, direction = 'increasing'))\n",
    "other = go.Histogram(x = df.loc[dfother,'rtl'].astype(int, 'ignore'), name = 'other',xbins =dict(size=1)\n",
    "                                  #,histnorm='probability'\n",
    "                     , cumulative=dict(enabled = False\n",
    "                                                                           , direction = 'increasing'))\n",
    "data = [jarrovii, virgatus,other]\n",
    "layout = go.Layout(\n",
    "    title = 'Histogram of rtl by species',\n",
    "    titlefont = dict(\n",
    "        size = 20),\n",
    "    xaxis = dict(\n",
    "        dtick = 1,\n",
    "        title = 'rtl (mm)',\n",
    "        titlefont = dict(\n",
    "            size = 18)),\n",
    "    yaxis = dict(\n",
    "        title = 'Number of Lizards',\n",
    "        titlefont = dict(\n",
    "            size = 18))\n",
    ")\n",
    "fig = go.Figure(\n",
    "        data = data,\n",
    "        layout = layout)\n",
    "py.iplot(fig, filename = 'Histogram of rtl by species (new)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it's worth inspecting values greater than 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_dfabove50 = (df.species.str.contains('j|v'))&(~df.rtl.isin(badtypes))\\\n",
    "&(df.loc[(~df.rtl.isin(badtypes)),'rtl'].astype(int, 'ignore')>=50)\n",
    "df[idx_dfabove50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outstanding1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these values are reasonable, but there are few for which we will need to go back to the field notes in 2011.  Those rows in which rtl > tl need to be investigated.\n",
    "\n",
    "[Back to Outstanding Problems](#outstanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_rtltlbig = (idx_dfabove50)&(df.rtl.astype(int,errors = 'ignore')>df.tl.astype(int,errors ='ignore'))\n",
    "df.loc[idx_rtltlbig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These appear to be cases where svl,tl,rtl and mass may have been entered into the wrong columns, i.e. the correct placement of current values-->correct column should probably be:\n",
    "- svl-->mass\n",
    "- tl-->svl\n",
    "- rtl-->tl\n",
    "- mass-->rtl\n",
    "\n",
    "We will correct these now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def swap(df):\n",
    "    tmp = {\n",
    "        'rtl':copy.copy(df['rtl']),\n",
    "        'tl':copy.copy(df['tl']),\n",
    "        'svl':copy.copy(df['svl']),\n",
    "        'mass':copy.copy(df['mass'])\n",
    "    }\n",
    "#     print(tmp)\n",
    "    df['rtl'] = tmp['mass']\n",
    "    df['tl'] = tmp['rtl']\n",
    "    df['svl'] = tmp['tl']\n",
    "    df['mass'] = tmp['svl']\n",
    "#     print(tmp)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_rtltlbig,['svl','rtl','tl','mass']] = swap(df.loc[idx_rtltlbig,['svl','rtl','tl','mass']])\n",
    "df.loc[idx_rtltlbig,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we force rtl to int type, ignoring errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rtl'] = df.rtl.astype(int,errors = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tl\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "Here we investigate and clean values in the column 'tl'. These should be int type values that are positive.  First, we test to see if all of the values are of type int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tl.astype(int,errors='ignore').apply(lambda x: type(x)).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the entries for which attempting to convert 'tl' results in a float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_floatNaNtl = df.tl.astype(int,errors='ignore').apply(lambda x: type(x) is float)\n",
    "df.loc[idx_floatNaNtl,'tl'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all NaN entries and can be ignored for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the non NaN entries now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_strtl = df.tl.astype(int,errors='ignore').apply(lambda x: type(x) is str)\n",
    "df.loc[idx_strtl,'tl'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of the value '56 (42)', the tl values that are not NaN could be converted to int types.  Let's inspect this entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',1000)\n",
    "idx_5642tl = df.loc[(idx_strtl) & (df.tl=='56 (42)'),:].index\n",
    "df.loc[df.index.isin(idx_5642tl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the notes in the misc column, tl should be recorded as 56.  We will do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index.isin(idx_5642tl),'tl']='56'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the entry looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index.isin(idx_5642tl),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a histogram to try and identify abnormalities among the other tl values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfnobadtypes0neg1 = (~df.tl.isin(badtypes))&(~df.tl.isin(['0','-1']))\n",
    "dfother = ~(df.species.dropna().str.contains('v|j'))&(df.species.notna())&(df.tl.notna())\n",
    "jarrovii = go.Histogram(x = df.loc[(df.species.str.contains('j'))&(df.tl.notna())\n",
    "                                   ,'tl'].astype(int, 'ignore'),name = 'S. jarrovii',xbins =dict(size=1)\n",
    "                        #,histnorm='probability'\n",
    "                        , cumulative=dict(enabled = False, direction = 'increasing'))\n",
    "virgatus = go.Histogram(x = df.loc[(df.species.str.contains('v'))&(df.tl.notna())\n",
    "                                   ,'tl'].astype(int, 'ignore'), name = 'S. virgatus',xbins =dict(size=1)\n",
    "                       #,histnorm='probability'\n",
    "                        , cumulative=dict(enabled = False, direction = 'increasing'))\n",
    "other = go.Histogram(x = df.loc[dfother,'tl'].astype(int, 'ignore'), name = 'other',xbins =dict(size=1)\n",
    "                                  #,histnorm='probability'\n",
    "                     , cumulative=dict(enabled = False\n",
    "                                                                           , direction = 'increasing'))\n",
    "data = [jarrovii, virgatus, other]\n",
    "layout = go.Layout(\n",
    "    title = 'Histogram of tl by species',\n",
    "    titlefont = dict(\n",
    "        size = 20),\n",
    "    xaxis = dict(\n",
    "        dtick = 1,\n",
    "        title = 'tl (mm)',\n",
    "#         tickfont = dict(\n",
    "#         size = 8),\n",
    "#         tickangle = 85,\n",
    "        titlefont = dict(\n",
    "            size = 18)),\n",
    "    yaxis = dict(\n",
    "        title = 'Number of Lizards',\n",
    "        titlefont = dict(\n",
    "            size = 18))\n",
    ")\n",
    "fig = go.Figure(\n",
    "        data = data,\n",
    "        layout = layout)\n",
    "py.iplot(fig, filename = 'Histogram of tl by species (new)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now there is not much we can identify graphically.  We will revist this later.  For now we will force tl to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tl'] = df.tl.astype(int, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svl\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a similar approach for svl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.svl.astype(int,errors='ignore').apply(lambda x: type(x)).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the entries for which attempting to convert 'svl' results in a float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_floatNaNsvl = df.svl.astype(int,errors='ignore').apply(lambda x: type(x) is float)\n",
    "df.loc[idx_floatNaNsvl,'svl'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all NaN entries and can be ignored for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the non NaN entries now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_strsvl = df.svl.astype(int,errors='ignore').apply(lambda x: type(x) is str)\n",
    "df.loc[idx_strsvl,'svl'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 'large', 'small', and '~70' require closer inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_txtvals = df.svl.isin(['small','large','~70'])\n",
    "df.loc[idx_txtvals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these values for svl should be set to NaN since these are estimates, not measured values.  For the entry with the svl value of '~70', we can add the estimated value to the misc column. We will use the [appendstr](#appendstr) function here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_apprx70svl = (idx_txtvals)&(df.svl=='~70')\n",
    "df.loc[idx_apprx70svl,'misc'] = df.loc[idx_apprx70svl,'misc'].apply(lambda x: appendstr(x,connector=';'\n",
    "                                                        ,position='end'\n",
    "                                                        ,value='svl extimated to be ~70mm'))\n",
    "df.loc[idx_apprx70svl,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_txtvals,'svl']=np.nan\n",
    "df.loc[idx_txtvals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we force the remaining svl values to int type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['svl'] = df.svl.astype(int, errors= 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autotomized \n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "[creating 'rtl_orig'and relabeling 'rtl' and 'autotomized](#creating-'rtl_orig'-and-relabeling-'rtl'-and-'autotomized')\n",
    "- [copy the values in rtl to a new column, *rtl_orig*](#copy-the-values-in-rtl-to-a-new-column)\n",
    "- [relabel entries in the autotomized column based on the values in the rtl_orig column](#relabel-entries-in-the-autotomized-column-based-on-the-values-in-the-rtl_orig-column) \n",
    "- [relabel entries in the rtl column](#relabel-entries-in-the-rtl-column)\n",
    "\n",
    "Here we populate the 'autotomized' column based on the values in 'rtl'.  Most of the source files did not have this category and have NaN values others have float values of 1.0, 2.0 or 3.0 for intact, autotomized with no regrowth or autotomized with regrowth, respectively.  The cleaned data for autotomized will contain  bool type values True, for having experienced auttomy (irrespective of regrowth) and False for having no evidence of havign experienced autotomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.autotomized.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will inspect the rtl values for entries with non NaN values for autotomized to determine if we can depend on rtl values to determine autotomy status.  In order to rely on rtl values, the following conditions must be met:\n",
    "- all entries in which autotomized equals 1.0 must have 0 for rtl\n",
    "- all entries in which autotomized equals 2.0 or 3.0 must have -1 or some value >0 for rtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intact = df.loc[(df.autotomized==1),'rtl'].astype(int,errors = 'ignore').value_counts(dropna=False)\n",
    "values2check = [x for x in intact.index[intact.index!=0]]\n",
    "if len(values2check)>0:\n",
    "    print(\"The rtl values associated with {} need a closer look.\".format(values2check))\n",
    "else:\n",
    "    print(\"Values for 'intact' entries are as expected.  Continue.\")\n",
    "pd.set_option('max_colwidth',1000)\n",
    "# df.loc[(df.autotomized==1)&(df.rtl.isin(['21'])),:]\n",
    "df.loc[(df.autotomized==1)&(df.rtl.astype(int, errors = 'ignore').isin([str(x) for x in values2check])),:]\n",
    "# need to see what broke this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='outstanding2'></a>\n",
    "\n",
    "[Back to Outstanding Problems](#outstanding)\n",
    "\n",
    "This lizard appears to have been misrecorded and should be listed as autotomized given the amount of regrowth. This should be confirmed in the field notes. If we trust the data as recorded and depend on the rtl values to label autotomized this will be corrected, so for now we will leave this as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotomized = df.loc[(df.autotomized==2),'rtl'].value_counts(dropna=False)\n",
    "aut_values2check = [x for x in autotomized.index[autotomized.index!='-1']]# change to 'isin' aregument with 0 and -1\n",
    "if len(aut_values2check)>0:\n",
    "    print(\"{} values associated with an rtl value of {} need a closer look.\"\\\n",
    "          .format(df.loc[(df.autotomized==2)&(df.rtl.isin(aut_values2check)),:].shape[0],aut_values2check))\n",
    "else:\n",
    "    print(\"Values for 'autotomized' entries are as expected.  Continue.\")\n",
    "pd.set_option('max_colwidth',1000)\n",
    "idx_aut_entries2check = (df.autotomized==2)&(df.rtl.isin([str(x) for x in aut_values2check]))\n",
    "df.loc[idx_aut_entries2check,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'outstanding3'></a>\n",
    "\n",
    "[Back to Outstanding Problems](#outstanding)\n",
    "\n",
    "Some of these cases are very straight forward given that the ratio of svl to tl is very close to 1, but others would be worth checking the original data to confirm. Another option is to use the svl to tl ratio of animals that we are sure are intact to decide how to classify these.  For now we will trust the system of recording used in 2010 and update the rtl values to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_aut_entries2check,'rtl'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_aut_entries2check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrown = df.loc[(df.autotomized==3),'rtl'].value_counts(dropna=False).reset_index()['index']\\\n",
    ".astype(int, errors = 'ignore')\n",
    "values2check = [x for x in regrown<=0]\n",
    "if sum(values2check)>0:\n",
    "    print(\"The values associated with {} need a closer look.\".format(values2check))\n",
    "else:\n",
    "    print(\"Values for 'regrown' entries are as expected.  Continue.\")\n",
    "pd.set_option('max_colwidth',1000)\n",
    "df.loc[(df.autotomized==3)&(df.rtl.isin([str(x) for x in values2check])),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries labeled as a 3.0 in the autotomized column do not appear as though their rtl values will present an issue for calculating new autotomized values.  We will leave these as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rtlRTL_ORIGautotomized'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating 'rtl_orig' and relabeling 'rtl' and 'autotomized'\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "[Back to: 'autotomized'](#autotomized)\n",
    "\n",
    "Now we will:\n",
    "- [copy the values in rtl to a new column, *rtl_orig*](#copy-the-values-in-rtl-to-a-new-column)\n",
    "- [relabel entries in the autotomized column based on the values in the rtl_orig column](#relabel-entries-in-the-autotomized-column-based-on-the-values-in-the-rtl_orig-column) \n",
    "- [relabel entries in the rtl column](#relabel-entries-in-the-rtl-column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='copyrtl'></a>\n",
    "\n",
    "#### copy the values in rtl to a new column\n",
    "[Back to: 'autotomized'](#autotomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rtl_orig'] = df.rtl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relabel entries in the autotomized column based on the values in the rtl_orig column\n",
    "[Back to: 'autotomized'](#autotomized)\n",
    "\n",
    "We will do this using the following logic:\n",
    "    - if rtl_orig !=0 & rtl_orig.notna(), autotomized = True\n",
    "    - if rtl_orig ==0, automized = False\n",
    "    - if rtl_orig.isna(), autotomized = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_auttrue = (~df.rtl_orig.isin(['0']))&(df.rtl_orig.notna())\n",
    "df.loc[idx_auttrue,'autotomized'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_autfalse = (df.rtl_orig.isin(['0']))&(df.rtl_orig.notna())\n",
    "df.loc[idx_autfalse,'autotomized'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_autnan = df.rtl_orig.isna()\n",
    "df.loc[idx_autnan,'autotomized'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relabel entries in the rtl column\n",
    "[Back to: 'autotomized'](#autotomized)\n",
    "We will do this using the following logic:\n",
    "    - if rtl_orig == -1, rtl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_rtlneg1 = df.rtl_orig=='-1'\n",
    "df.loc[idx_rtlneg1,'rtl'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.autotomized.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toes \n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Cleaning](#CleaningData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make changes to toes based on comments regarding a 2004 male Sv with toes recorded as '1-6-16-17-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sv2004m16161720 = (df.species=='sv') \\\n",
    "& (df.sex=='m') \\\n",
    "& (df.date.str.contains('2004-07-04')) \\\n",
    "&(df.svl=='52')&(df.tl=='53')\n",
    "\n",
    "df.loc[idx_sv2004m16161720]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_sv2004m16161720,'toes'] = '1-7-16-17-20'\n",
    "df.loc[idx_sv2004m16161720,'rtl'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this entry looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_sv2004m16161720]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting toes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will rename \"toes\" to \"toes_orig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'toes':'toes_orig'},index = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new column, \"toes\"  for the renamed toes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toes'] = df.toes_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we attempt to identify problem _toes_ values and either correct them or export them for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pattern1 = \".( {1,}-.|.- {1,}.)\" # toes entries with any number of spaces on either side of a hyphen\n",
    "pattern2 = \".( {,}\\w{,} {1,}).\" # toes entries with space around or between numbers <- the spaces here should be deleted\n",
    "pattern3 = \".(').\"\n",
    "pattern4 = \"./.\"  # entries with '/' <-- need to replace these with '-'\n",
    "pattern5 = \"(\\?{1,})\"#<-- these needs to be investigated\n",
    "pattern6 = \"^\\d{3,}$\" # entries consist of only a single number comprised of at least three digits \n",
    "#<-- these needs to be investigated by checking raw field notes\n",
    "pattern7 = \".(-{2,}).\" # entries which have at least 2 consecutive '-' <- these should be investigated\n",
    "pattern8 = \"^0\" # entries in which single digit numbers have a leading \"0\" <-- Check raw field notes on this too\n",
    "pattern9 = \"a\\w\" #<--handled hyphens should be inserted  between the [ab] and \\w \n",
    "# entries that contain an 'a' or 'b' followed by any character in the set [a-zA-Z0-9_]\n",
    "pattern10 = \"b\\w\" #<--handled hyphens should be inserted  between the [ab] and \\w \n",
    "pattern11 = \"\\wa\" # entries that contain an 'a' or 'b' preceded by any character in the set [a-zA-Z0-9_]\n",
    "pattern12 = \"\\wb\" # entries that contain an 'a' or 'b' preceded by any character in the set [a-zA-Z0-9_]\n",
    "pattern13 = \"[()]\"\n",
    "# remove space before 'a' at end of toes\n",
    "#investigate '\\d-', \n",
    "#'-(*)-', \n",
    "#' (16) ', \n",
    "#'---', <- may not exist in raw data\n",
    "#'\\d- ', \n",
    "#'- \\d', \n",
    "#transcription errors from excel (toes in date format,\n",
    "#'-\\d\\d\\d\\d' <- may not be in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to change this block if we add or remove toe patterns.\n",
    "This is not ideal and needs to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_pattern = pd.Series([*range(1,14)]) \n",
    "toe_pattern = make_str(toe_pattern)\n",
    "# print(toe_pattern,\"\\n\\n\")\n",
    "\n",
    "toe_pattern_descr = pd.Series([pattern1,pattern2,pattern3,pattern4\n",
    "                               ,pattern5,pattern6,pattern7,pattern8\n",
    "                               ,pattern9,pattern10,pattern11,pattern12,pattern13])\n",
    "toe_pattern_descr = toe_pattern_descr.astype(str)\n",
    "# print(toe_pattern_descr,\"\\n\\n\")\n",
    "\n",
    "toe_pattern_reference = pd.DataFrame({'toe_pattern': toe_pattern,'description':toe_pattern_descr})\n",
    "toe_pattern_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we replace the string 'nan' in the data set with a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.toes=='nan','toes'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many of these patterns we need to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toe_pattern'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a for-loop to label the patterns \n",
    "(there's probably a better way to do this with pandas map or apply, but I'll have to figure this out, for now this is fast enough, but it could make a difference with a larger data set or with more patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,toe_pattern_reference.shape[0]):\n",
    "    tmp_pat_num = toe_pattern_reference.iloc[i,0]\n",
    "    tmp_pattern = toe_pattern_reference.iloc[i,1]\n",
    "    df = label_pattern(x=df,pat_num=tmp_pat_num,pattern = tmp_pattern,pat_col = 'toe_pattern',col = 'toes')\n",
    "#     print(df.toe_pattern.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df.toes[random.randint(10,df.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick summary of the number of observations for each pattern in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_errors =df.toe_pattern.value_counts(dropna=False).reset_index()\\\n",
    ".rename(columns = {'index':'toe_pattern','toe_pattern':'observations'})\n",
    "toe_errors.loc[toe_errors.toe_pattern.isnull(),'toe_pattern'] = 'Not covered by current patterns'\n",
    "toe_errors_desc = toe_errors.merge(toe_pattern_reference,'left',on='toe_pattern')\n",
    "toe_errors_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure we've accounted for every row in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accountedRows = toe_errors.observations.sum()\n",
    "totalRows = df.shape[0]\n",
    "notAccountedRows = df.shape[0] - toe_errors.observations.sum()\n",
    "print(\"\\nThere are {} rows accounted for in the patterns (including null values) and there {} rows in the full data set.\\\n",
    "  There are {} rows unaccounted for.\".format(accountedRows,totalRows,notAccountedRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we correct these patterns. We'll preserve the original toe data in a column called \"toes_orig\" just in case.  We can drop this later, if we are comfortable with the changes.  The new toes will be labeled \"toes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections_config = {'01':{'action':'replace','pattern_b':\" - \",'replacement':\"-\"},\n",
    "            '02':{'action':'replace','pattern_b':\" \",'replacement':\"-\"},\n",
    "            '03':{'action':'replace','pattern_b':\"\\'\",'replacement':\"\\\"\\\"\"},\n",
    "            '04':{'action':'replace','pattern_b':\"/\",'replacement':\"-\"},\n",
    "            '05':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '06':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '07':{'action':'save','pattern_b':np.nan,'replacement':np.nan},\n",
    "            '08':{'action':'replace','pattern_b':\"^0\",'replacement':\"\\\"\\\"\"},\n",
    "            '09':{'action':'replace','pattern_b':'a','replacement':'-a'},\n",
    "            '10':{'action':'replace','pattern_b':'b','replacement':'-b'},          \n",
    "            '11':{'action':'replace','pattern_b':\"a\",'replacement':\"a-\"},\n",
    "            '12':{'action':'replace','pattern_b':\"b\",'replacement':\"b-\"},\n",
    "            '13':{'action':'replace','pattern_b':\"[()]\",'replacement':\"\\\"\\\"\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toe_errors_desc['action'] = toe_errors_desc.loc[toe_errors_desc.toe_pattern.str.len()==2].toe_pattern\\\n",
    ".map(lambda x: corrections_config[x]['action'])\n",
    "\n",
    "toe_errors_desc['pattern_b'] = toe_errors_desc.loc[toe_errors_desc.toe_pattern.str.len()==2].toe_pattern\\\n",
    ".map(lambda x: corrections_config[x]['pattern_b'])\n",
    "\n",
    "toe_errors_desc['replacement'] = toe_errors_desc.loc[toe_errors_desc.toe_pattern.str.len()==2].toe_pattern\\\n",
    ".map(lambda x: corrections_config[x]['replacement'])\n",
    "\n",
    "toe_errors_desc = toe_errors_desc.sort_values('toe_pattern').reset_index(drop=True)\n",
    "toe_errors_desc.loc[toe_errors_desc['toe_pattern'] == 'Not covered by current patterns',\n",
    "                                    ['action']] = 'ignore'\n",
    "toe_errors_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('toe_pattern').head().merge(toe_errors_desc,on='toe_pattern')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(0,toe_errors_desc.shape[0]):\n",
    "    tmp_pat_num = toe_errors_desc.iloc[i,0]\n",
    "    tmp_pattern = toe_errors_desc.iloc[i,2]\n",
    "    action = toe_errors_desc.iloc[i,3]\n",
    "    tmp_replacement = toe_errors_desc.iloc[i,4]\n",
    "    tmp_x = df.loc[df.toe_pattern==tmp_pat_num,:]\n",
    "    \n",
    "    if action =='save':\n",
    "        tmp_filename = 'pattern'+tmp_pat_num+'.csv'\n",
    "        tmp_x.to_csv(tmp_filename)\n",
    "        print(\"Pattern {} successfully saved to {}.\".format(tmp_pattern,tmp_filename))\n",
    "    elif action =='ignore':\n",
    "        df.loc[df.toe_pattern==tmp_pat_num,'toes'] = df.loc[df.toe_pattern==tmp_pat_num,'toes_orig']\n",
    "        print(\"Pattern {} ignored.\".format(tmp_pattern))\n",
    "    elif action =='replace':\n",
    "        df.loc[df.toe_pattern==tmp_pat_num,'toes'] = replace_pattern(x=df.loc[df.toe_pattern==tmp_pat_num]\n",
    "                                                                     ,pattern = tmp_pat_num\n",
    "                                                                     ,pattern_b = tmp_pattern\n",
    "                                                                     ,source_col = 'toes'\n",
    "                                                                    ,replacement = tmp_replacement)\n",
    "        print(\"Pattern {} successfully replaced with {}.\".format(tmp_pattern,tmp_replacement))\n",
    "    else:\n",
    "        print(\"No direction provided for pattern {}.  No action was taken.\".format(tmp_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pattern2(x, pattern_b, replacement):\n",
    "    \"\"\"searches a pandas series for a regex expression, pattern, and replaces with replacement\"\"\"\n",
    "\n",
    "    try:\n",
    "        res = x.strip().replace(pattern_b,replacement)\n",
    "    except:\n",
    "        res = x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,toe_errors_desc.shape[0]):\n",
    "    tmp_pat_num = toe_errors_desc.iloc[i,0]\n",
    "    tmp_pattern = toe_errors_desc.iloc[i,2]\n",
    "    pattern_b = toe_errors_desc.iloc[i,5]\n",
    "    action = toe_errors_desc.iloc[i,3]\n",
    "    tmp_replacement = toe_errors_desc.iloc[i,4]\n",
    "    tmp_x = df.loc[df.toe_pattern==tmp_pat_num,:]\n",
    "    \n",
    "    if action =='save':\n",
    "        tmp_filename = 'pattern'+tmp_pat_num+'.csv'\n",
    "        tmp_x.to_csv(tmp_filename)\n",
    "        print(\"Pattern {} successfully saved to {}.\".format(tmp_pattern,tmp_filename))\n",
    "    elif action =='ignore':\n",
    "        df.loc[df.toe_pattern==tmp_pat_num,'toes'] = df.loc[df.toe_pattern==tmp_pat_num,'toes_orig']\n",
    "        print(\"Pattern {} ignored.\".format(tmp_pattern))\n",
    "    elif action =='replace':\n",
    "        df.loc[df.toe_pattern==tmp_pat_num,'toes'] = replace_pattern(x=df.loc[df.toe_pattern==tmp_pat_num]\n",
    "                                                                     ,pattern_b = tmp_pattern\n",
    "                                                                     ,source_col = 'toes'\n",
    "                                                                    ,replacement = tmp_replacement)\n",
    "        print(\"Pattern {} successfully replaced with {}.\".format(tmp_pattern,tmp_replacement))\n",
    "    else:\n",
    "        print(\"No direction provided for pattern {}.  No action was taken.\".format(tmp_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "' 8 - 12 - 20'.strip().replace(' - ','-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patterns = [' ','-']\n",
    "Pattern_b = [' ',' - ']\n",
    "Source_col = 'toes'\n",
    "Replacement = '-'\n",
    "testtoes = ['8 12 20',' 8-12-20', '8 - 12 - 20']\n",
    "def replace_pattern2(x, pattern_b, replacement):\n",
    "    \"\"\"searches a pandas series for a regex expression, pattern, and replaces with replacement\"\"\"\n",
    "\n",
    "    try:\n",
    "        res = x.strip().replace(pattern_b,replacement)\n",
    "    except:\n",
    "        res = x\n",
    "    return res\n",
    "\n",
    "replace_pattern2(x=testtoes[2],pattern_b = Pattern_b[1],replacement = Replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to ensure that the patterns are being handled appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['toes_orig','toes','toe_pattern']].groupby('toe_pattern').head().merge(toe_errors_desc,on='toe_pattern').\\\n",
    "toes_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['toes_orig','toes','toe_pattern']].groupby('toe_pattern').head().merge(toe_errors_desc,on='toe_pattern')\\\n",
    ".toes_orig.apply(len).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that none of the replacements are working as expected.  We wil need to make some changes to the replace_pattern function or to how it's applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we confirm that the patterns we expect to have eliminated have indeed been eliminated from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,toe_pattern_reference.shape[0]):\n",
    "    tmp_pattern = str(toe_pattern_reference.iloc[i,1])\n",
    "    print(report_pattern(df,tmp_pattern,'toes','Post-Correction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as expected since we left toe pattern 5 uncorrected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sex'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sex\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "Next we move on to cleaning the \"sex\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to get an idea of the types of problems in the sex column.  We start by striping leading and trailing whitespaces.  You can see here that there were none in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sex.str.len().unique())# returns unique lengths of sex\n",
    "df.sex=df.sex.str.strip()\n",
    "print(df.sex.str.len().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify non \"m\" or \"f\" values and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patterns_sex=\"m|f|NA\"\n",
    "non_matches=df.sex.loc[df.sex.str.match(patterns_sex)!=True]\n",
    "print(\"\\nThere are {} entries for sex which do not match the patterns {}:\"\\\n",
    "      .format(non_matches.shape[0],patterns_sex.split(\"|\")))\n",
    "non_matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify values to convert to NA, m, or f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex2NA = ['adult','juv','nan','\\?\\?\\?','\\?']\n",
    "sex2m = ['unm','M']\n",
    "sex2f = ['F']\n",
    "df.loc[df.sex.isin(sex2NA)==True]\n",
    "print(\"There are {} entries that should be converted to 'NaN'\".format(df.sex.isin(sex2NA).sum()))\n",
    "print(\"There are {} entries that should be converted to 'm'\".format(df.sex.isin(sex2m).sum()))\n",
    "print(\"There are {} entries that should be converted to 'f'\".format(df.sex.isin(sex2f).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the values to NA, f, or m, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.sex.isin(sex2NA),'sex']=np.nan\n",
    "df.loc[df.sex.isin(sex2m),'sex']='m'\n",
    "df.loc[df.sex.isin(sex2f),'sex']='f'\n",
    "print(\"Now there are {} entries that should be converted to 'NaN'\".format(df.sex.isin(sex2NA).sum()))\n",
    "print(\"Now there are {} entries that should be converted to 'm'\".format(df.sex.isin(sex2m).sum()))\n",
    "print(\"Now there are {} entries that should be converted to 'f'\".format(df.sex.isin(sex2f).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set all remaining sex with \"?\" to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.sex.str.contains('\\?')) & (df.sex.notnull()),'sex'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'species'></a>\n",
    "\n",
    "### Species\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.species.str.len().unique())# returns unique lengths of species\n",
    "df.species=df.species.str.strip()\n",
    "print(df.species.str.len().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_species=\"j|v|sj|sv|NA\"\n",
    "idx_notsjsv = (df.species.str.match(patterns_species)!=True)&(df.species.str.contains('j|v',case=False)!=True)\n",
    "non_matches=df.species.loc[idx_notsjsv]\n",
    "print(\"\\nThere are {} entries for species which do not match the patterns {} and are unlikely to be definitely \\\n",
    "'sv', 'sj':\".format(non_matches.shape[0],patterns_species.split(\"|\")))\n",
    "non_matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set species for these entries to  'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.species.isin(non_matches.unique()),'species'] = 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='outstanding4'></a>\n",
    "#### *Sceloporus jarrovii*\n",
    "[Back to Outstanding Problems](#outstanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.species.str.contains('j',case=False),'species'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values with '?' should be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sjrev = df.species.str.contains('j',case=False)&(df.species.str.contains('\\?'))\n",
    "df.loc[idx_sjrev,'species'].value_counts(dropna=False)\n",
    "df.loc[idx_sjrev,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sj = df.species.str.contains('j',case=False)&(~df.species.str.contains('\\?'))\n",
    "df.loc[idx_sj,'species'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the others should be converted to 'j'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_sj,'species'] = 'j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Sceloporus virgatus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sv = df.species.str.contains('v',case=False)\n",
    "df.loc[idx_sv,'species'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert these to 'v'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_sv,'species'] = 'v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new.recap\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "1. [potential new and recap](#newandrecap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newKeep = ['new','n','NEW','N','New']\n",
    "recapKeep = ['recap','r','Recap','R']\n",
    "sighting = ['sighting','sighted','heard','sighted ', 'missed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's identify other values to include in each list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'newandrecap'></a>\n",
    "#### potential new and recap\n",
    "\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)\n",
    "\n",
    "[Back to: new.recap](#newrecap)\n",
    "\n",
    "Let's identify cases that may be classified as recaptures or new captures.  We'll start with new captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_potnew = df['new.recap'].str.contains('n',case=False)==True\n",
    "print([x for x in df.loc[idx_potnew,'new.recap'].value_counts(dropna=False).index])\n",
    "df.loc[idx_potnew,'new.recap'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'NEW', 'New', 'new ', 'new', and 'N' are certainly new captures and should be converted to 'N'. for now we will add them to the *newKeep* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newKeep = list(set(newKeep + ['NEW', 'New', 'new ', 'new', 'N']))\n",
    "newKeep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'sighting', '?sighting', \"didn't catch\", 'sighing', and 'not caught' should be sightings, so wee will add them to the list of *sighting* list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting = list(set(sighting + ['sighting', '?sighting', 'didn\\'t catch', 'sighing', 'not caught']))\n",
    "sighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we take a similar approach for recaptures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_potrec = df['new.recap'].str.contains('r',case=False)==True\n",
    "print([x for x in df.loc[idx_potrec,'new.recap'].value_counts(dropna=False).index])\n",
    "df.loc[idx_potrec,'new.recap'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'recap', 'r', 'recap ',  'reecap', 'recapq' are certainly recaptures and should be converted to 'R'. for now we will add them to the *newRecapKeep* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recapKeep = list(set(recapKeep + ['recap', 'r', 'recap ',  'reecap', 'recapq']))\n",
    "recapKeep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'recap?', 'recap/new', 'recap? - toes suggest a NEW mark','recap ?', 'r?' require inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_recaprev = df['new.recap']\\\n",
    ".isin(['recap?','recap/new', 'recap ?', 'recap? - toes suggest a NEW mark', 'r?'])\n",
    "df.loc[idx_recaprev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'visual recapture', and 'heard' should be reclassified as 'sighting', so we will add them to the *sighting* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting = list(set(sighting + ['visual recapture', 'heard']))\n",
    "sighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign 'N' to confired new captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_new = df['new.recap'].isin(newKeep)\n",
    "df.loc[idx_new,'new.recap'] = 'N'\n",
    "df.loc[idx_new,'new.recap'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign 'R' to confirmed recaptures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_recap = df['new.recap'].isin(recapKeep)\n",
    "df.loc[idx_recap,'new.recap'] = 'R'\n",
    "df.loc[idx_recap,'new.recap'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign 'S' to confirmed sightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sighting = df['new.recap'].isin(sighting)\n",
    "df.loc[idx_sighting,'new.recap'] = 'S'\n",
    "df.loc[idx_sighting,'new.recap'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining *new.recap* values need inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[~df['new.recap'].isin(['R','N','S']),'new.recap'].unique())\n",
    "idx_2check = ~df['new.recap'].isin(['R','N','S'])\n",
    "df.loc[idx_2check,'new.recap'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} entries which need to be checked and classfied.\"\\\n",
    "      .format(df.loc[(idx_2check) & (df['new.recap'].notna()) ].shape[0]))\n",
    "df.loc[(idx_2check) & (df['new.recap'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Cleaning](#CleaningData)\n",
    "\n",
    "[Back to: Column-by-Column Cleaning](#Column-by-Column-Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to handle the date data which don't confirm to a typical date format (e.g., 2010 data which contain roman numerals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_romdate = (df.date.notna())&(df.date.str.contains('i|v|x'))\n",
    "df.loc[idx_romdate,'date'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to replace the values in the date strings with the roma month surrounded by hyphens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[idx_romdate,'date'] = df.loc[idx_romdate,'date'].apply(lambda x: rom2arab(x))\n",
    "# .apply(lambda x: pd.to_datetime(rom2arab(x),errors='ignore'))\n",
    "df.loc[idx_romdate,'date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the replaced values look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~idx_romdate,'date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'resumehere'></a>\n",
    "\n",
    "[Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_romdate,'date'] = df.loc[idx_romdate].date.apply(pd.to_datetime)\n",
    "# some weird stuff is happening when I try to assign these values to the object their supposed to be in\n",
    "# I'll have to figure this out later. Right now this only affects 2010 data, so I'll move on\n",
    "df.loc[idx_romdate,'date'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Class of Columns\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Cleaning](#CleaningData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert integer columns to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intCols = ['meters']\n",
    "df[intCols]=df[intCols].astype(int,errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert numeric columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = ['svl','tl','rtl','mass']\n",
    "df[numCols]=df[numCols].apply(pd.to_numeric,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string columns to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strCols = ['toes','sex','species','vial']\n",
    "df[strCols]=df[strCols].astype(str, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.date==\"NA\"]=np.nan\n",
    "df.date = pd.to_datetime(df.date,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nAfter applying the above changes, the data types are as follows:\\n{}\".format(df.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AddVar1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding variables [*year*](#year) and [*rtl_orig*](#rtlorig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='year'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year\n",
    "[Back to: Top](#Table-of-Contents)\n",
    "\n",
    "[Back to: Adding variables](#AddVar1)\n",
    "\n",
    "We will use data contained in the *date* column to create the variable *year*.  TO do this we will define a small function, [*myint*](#myint), to convert year to an int type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply [*myint*](#myint) to the 'date' column to create the variable year and inspect the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.date.dt.year.apply(myint,verbose=False)\n",
    "df.year.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the entries with 'nan' values.  Note these 'nan' values are string values and not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.year=='nan',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_nntoes = (df.year=='nan')&(df.toes_orig.notna())&(df.toes_orig!='nan')\n",
    "print(\"{} of these entries have non-null values in the originl toes column.  Let's inspect these\"\\\n",
    "      .format(df.loc[idx_nntoes].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_nntoes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these appear to be from 2016/17, based on the toe vial numbers.  one may be from 2018, but we will need to confirm and determine if any of these have sufficient information to keep them in the data set.  For now we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx_nntoes] = None\n",
    "df.loc[idx_nntoes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding New Columns\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "We need to add new columns which we will use later in analyses:\n",
    "- [TL_SVL](#TL-SVL)\n",
    "- [Mass_SVL](#Mass_SVL)\n",
    "- [Lizard Number](#Lizard-Number)\n",
    "     - [assign lizard numbers](#Assign) \n",
    "     - [QC the lizard numbers](#QcLizNum) \n",
    "- [Days Since Capture](#daysSinceCapture)\n",
    "- [Number of Captures](#capture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL_SVL \n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tl_svl']=(df.tl/df.svl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass_SVL\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mass_svl']=(df.mass/df.svl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lizard Number\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n",
    "Here we use a set of functions to:\n",
    " - [Assign Lizard Numbers](#Assign-Lizard-Numbers) to unique individuals (we repeat this step to ensure we have assigned all animals a number) and \n",
    " - [QC the Numbers](#QC-the-Numbers) assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Assign'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Lizard Numbers\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a first attempt at assigning lizard numbers.  We use the *lizsort* function to identify the subset of rows from the original dataset which have sufficient information to allow us to make an automated decision about the uniqueness of the individuals identified in those rows.  We name that df *sortable*.  The unsortable data are saved to a path as a file, *unsortable.csv*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortable = lizsort(df, path = sourceinterDataBig)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the *mindate* function on *sortable*.  This identifies the earliest date at which each unique combination of *sortCriteria* are recorded in a new column, *initialCaptureDate*.  The default sortCriteria are of the variables *species*, *toes*, and *sex*.  This also calculates and adds a column for *year_diff*, the difference in years between the initial capture date and the date value in a given row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortable = mindate(sortable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call a the function *smallest*, which is analogous to *mindate*, but groups data in *sortable* into unique combinations of *species*, *toes*, *sex*, and *initialCaptureDate* before assigning the smallest SVL value recorded for each group to a new column for that group, *smallest_svl*.  *smallest* then calculates a new column *svl_diff* which is analogous to *year_diff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortable = smallest(sortable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the *validate* function on *sortable*, which applies a series of validation tests to the data, sequentially numbers unique combinations of *sortCriteria* and returns a dict containing uniquely numbered individuals and summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sort = validate(sortable)\n",
    "df_numbered1 = tmp_sort['val_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt to assign lizard numbers\n",
    "\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n",
    "Here we make a second attempt at assigning lizard numbers to ensure that all lizards have been assigned.  This second attempt is focused on those rows which were unvalidated during the first attempt *n_val_data*.  Since these are already a subset of those data which were sortable, we need only call the *mindate*, *smallest*, and *validate* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = mindate(tmp_sort['n_val_data'])\n",
    "n_val = smallest(n_val)\n",
    "df_numbered2 = validate(n_val)['val_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no rows remain unvalidated, we will not attempt a third validation.  We will simply append *df_numbered1* and *df_numbered2* to create *df_numbered* to create our full numbered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered = df_numbered1.append(df_numbered2,ignore_index=True,sort=False)\n",
    "print(\"df:{}\\ndf_numbered1:{}\\ndf_numbered2:{}\\ndf_numbered:{}\".format(df.shape,df_numbered1.shape,df_numbered2.shape,\n",
    "                                                               df_numbered.shape))\n",
    "df_numbered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QcLizNum'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Lizard Numbers\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we display the output data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify individuals that are missing any of the critical values, but have a lizard number assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_critical_na = (df_numbered.toes_orig.isna())|(df_numbered.species.isna())|(df_numbered.date.isna())\n",
    "print(\"There are {} rows that fit have na values in any category critical for liznumber generation.\\n{}\"\\\n",
    "      .format(df_numbered.loc[(idx_critical_na)].shape[0],df_numbered.loc[(idx_critical_na),\n",
    "                                                                          ['toes_orig','toes','species','date']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify individuals that have same species and toes, but different sex for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered = df_numbered.merge(df_numbered.groupby(['species','toes']).sex.nunique().reset_index()\\\n",
    "                       .rename(columns = {'sex':'sex_count'}),how = 'inner', on = ['species','toes'])\n",
    "df_numbered.loc[df_numbered.sex_count>1,:].to_csv('entries flagged with same species and toes diff sex.csv')\n",
    "print(\"{} rows have the same species and toes but different values for sex\"\\\n",
    "      .format(df_numbered.loc[df_numbered.sex_count>1,:].shape[0]))\n",
    "df_numbered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lizard Numbers in the sample range from {} to {}.\"\\\n",
    "      .format(df_numbered.liznumber.min(),df_numbered.liznumber.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possibleLizNum = set(range(int(df_numbered.liznumber.min()),int(df_numbered.liznumber.max())))\n",
    "actualLizNum = set(pd.Series(df_numbered.liznumber.unique()).dropna().apply(int))\n",
    "print(\"\\nThere are {} entries.  There are {} unique lizard numbers.\\\n",
    "\\n\\nThe liznumber ranges from {} to {}.\"\\\n",
    "  .format(df_numbered.shape[0],len(df_numbered.liznumber.unique())\\\n",
    "          ,df_numbered.liznumber.min(),df_numbered.liznumber.max()))\n",
    "\n",
    "missingLizNum = possibleLizNum - actualLizNum\n",
    "if len(missingLizNum)>0:\n",
    "    print(\"\\n\\nThe following numbers are not assigned to a lizard:\\n{}\"\\\n",
    "      .format(missingLizNum))\n",
    "else:\n",
    "    print(\"\\n\\nThere are no numbers which were not assigned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='daysSinceCapture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days Since Capture\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n",
    "*daysSinceCapture* identifies the number of days since the animal was captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered.loc[:,'daysSinceCapture'] = (df_numbered.date - df_numbered.initialCaptureDate).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='capture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Number\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Adding New Columns](#Adding-New-Columns)\n",
    "\n",
    "*capture* identifies the number of times an animal has been captured prior to an entry.\n",
    "We will need to [QC of Capture Number and Recap Status](#QC-of-Capture-Number-and-Recap-Status) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to QC this this seems to be leading to several cases in which recap individuals that \n",
    "# only have one capture\n",
    "df_numbered['capture'] = df_numbered.sort_values(['liznumber','date'])\\\n",
    ".groupby(['liznumber']).daysSinceCapture.cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_numbered.loc[df_numbered.species.isin(['j','v'])].groupby('capture').capture.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's QC these numbers by looking at the distribution of the number of captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Sj = go.Histogram(x = df_numbered.loc[(df_numbered.species=='j')].groupby('liznumber').capture.max(),name='Sj')\n",
    "Sv = go.Histogram(x = df_numbered.loc[(df_numbered.species=='v')].groupby('liznumber').capture.max(),name='Sv')\n",
    "\n",
    "data = [Sj,Sv]\n",
    "layout = go.Layout(\n",
    "    title = 'Histogram of Maximum Number of Captures for CC 2000-2017',\n",
    "    titlefont = dict(\n",
    "        size = 20),\n",
    "    xaxis = dict(\n",
    "        dtick = 1,\n",
    "        title = 'Maximum Number of Captures',\n",
    "        titlefont = dict(\n",
    "            size = 18)),\n",
    "    yaxis = dict(\n",
    "        title = 'Number of Unique Lizards',\n",
    "        titlefont = dict(\n",
    "            size = 18)))\n",
    "\n",
    "fig = go.Figure(\n",
    "        data = data,\n",
    "        layout = layout)\n",
    "py.iplot(fig, filename = 'Histogram of Maximum Number of Captures for CC 2000-2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll display all data points associated with lizards who have more than 8 captures reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 8\n",
    "immortals = df_numbered.loc[df_numbered.capture>threshold].liznumber.unique()\n",
    "df_immortals = df_numbered.loc[df_numbered.liznumber.isin(immortals)]\n",
    "print(\"There are {} lizards with greater than {} captures.  They account for {} rows of data.\\n\"\\\n",
    "      .format(df_immortals.liznumber.nunique(),threshold,df_immortals.shape[0]))\n",
    "df_immortals = df_immortals.loc[:,[ 'liznumber','capture','toe_pattern','species', 'toes_orig', 'toes',\n",
    "                                   'sex', 'date', 'svl_diff', 'tl', 'rtl_orig',\n",
    "       'meters', 'new.recap','sighting', 'vial', 'misc','initialCaptureDate','year_diff', 'sex_count',\n",
    "                  'daysSinceCapture','year']].sort_values(['liznumber','year'])\n",
    "print(\"\\nTheir toe_patterns are distributed as follows:\\n{}\".format(df_immortals.toe_pattern.value_counts(dropna=False)))\n",
    "print(\"\\nThe new toes for those lizards are distributed as follows:\\n{}\"\n",
    "      .format(df_immortals.groupby('toe_pattern').toes.value_counts(dropna=False)))\n",
    "print(\"\\nThose with NaN values assigned as toe_pattern have the following original toes:\\n{}\"\\\n",
    "      .format(df_immortals.loc[df_immortals.toe_pattern.isna()].toes_orig.value_counts(dropna=False)))\n",
    "# print(df_immortals.loc[df_immortals.toe_pattern.isna()].liznumber.value_counts(dropna=False))\n",
    "df_immortals.loc[df_immortals.toe_pattern.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the liz number generation failed since lizards with no toes have been assigned a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will export these for further inspection using the *[exportliz](#exportliz)* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportliz(df_immortals,iterator='liznumber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yearstoolarge'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### years too large\n",
    "[Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yeartoomuch = df_numbered.loc[df_numbered.year_diff>=5,'liznumber']\n",
    "checkyears = df_numbered.loc[df_numbered.liznumber.isin(yeartoomuch)].sort_values(['liznumber'])\n",
    "checkyears.to_csv('check years.csv')\n",
    "checkyears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarrovii = go.Histogram(x = df_numbered.loc[df_numbered.species.isin(['j'])].groupby('liznumber')\\\n",
    "                     .year_diff.max(),name = 'S. jarrovii')\n",
    "virgatus = go.Histogram(x = df_numbered.loc[df_numbered.species.isin(['v'])].groupby('liznumber')\\\n",
    "                     .year_diff.max(), name = 'S. virgatus')\n",
    "data = [jarrovii, virgatus]\n",
    "layout = go.Layout(\n",
    "    title = 'Number of Individuals by Years Between First and Last Capture 2000-2017',\n",
    "    titlefont = dict(\n",
    "        size = 20),\n",
    "    xaxis = dict(\n",
    "        dtick = 1,\n",
    "        title = 'Maximum Number of Years Since Initial Capture',\n",
    "        titlefont = dict(\n",
    "            size = 18)),\n",
    "    yaxis = dict(\n",
    "        title = 'Number of Lizards',\n",
    "        titlefont = dict(\n",
    "            size = 18))\n",
    ")\n",
    "fig = go.Figure(\n",
    "        data = data,\n",
    "        layout = layout)\n",
    "py.iplot(fig, filename = 'Frequency of Captures in Crystal Creek 2000 - 2017 (by species)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we drop this figure entirely.  Not sure this is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze work on this figure until we've resolved issues with calculation based on year\n",
    "# ADD HORIZONTAL LINES FOR EACH YEAR\n",
    "j_lizards = go.Scatter(x = df_numbered.loc[df_numbered.species.isin(['j'])].liznumber,\n",
    "                   y = df_numbered.loc[df_numbered.species.isin(['j'])]\\\n",
    "                      .groupby('liznumber').daysSinceCapture.max(), \n",
    "                     mode = 'markers', name='S. jarrovii')\n",
    "v_lizards = go.Scatter(x = df_numbered.loc[df_numbered.species.isin(['v'])].liznumber,\n",
    "                   y = df_numbered.loc[df_numbered.species.isin(['v'])]\\\n",
    "                      .groupby('liznumber').daysSinceCapture.max(), \n",
    "                     mode = 'markers', name='S. virgatus')\n",
    "# year1 = go.Scatter(x=[df_numbered.liznumber.min(),df_numbered.liznumber.max()],y = (365))\n",
    "# year2 = go.Scatter(y = 365*2)\n",
    "# year3 = go.Scatter(y = 365*3)\n",
    "# year4 = go.Scatter(y = 365*4)\n",
    "# year5 = go.Scatter(y = 365*5)\n",
    "# year6 = go.Scatter(y = 365*6)\n",
    "# year7 = go.Scatter(y = 365*7)\n",
    "# year8 = go.Scatter(y = 365*8)\n",
    "\n",
    "# data = [j_lizards, v_lizards, year1, year2, year3, year4, year5, year6, year7, year8]\n",
    "data = [j_lizards, v_lizards]\n",
    "layout = go.Layout(\n",
    "    title = 'Days Since Initial Capture in Crystal Creek 2000 - 2017',\n",
    "        titlefont = dict(\n",
    "            size = 20),\n",
    "    xaxis = dict(\n",
    "            title='Lizard Number',\n",
    "            titlefont=dict(\n",
    "                size=18)),\n",
    "    yaxis = dict(\n",
    "            title='Greatest Number of Days Since<br> Initial Capture',\n",
    "            titlefont=dict(\n",
    "                size=18)))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename = 'Days Since Initial Capture in Crystal Creek 2000 - 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop this figure as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfF = df_numbered.loc[(df_numbered.sex =='f' )& (df_numbered.species.isin(['j','v']))]\n",
    "dfM = df_numbered.loc[(df_numbered.sex =='m') & (df_numbered.species.isin(['j','v']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze work on this figure until we've resolved issues with calculation based on year\n",
    "females = go.Scatter(\n",
    "    x = dfF.liznumber,\n",
    "    y = dfF.groupby('liznumber').daysSinceCapture.max(),\n",
    "    name = 'females',\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = 'rgba(152, 0, 0, .8)',\n",
    "        opacity = 0.75,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "            color = 'rgb(0, 0, 0)'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "males = go.Scatter(\n",
    "    x = dfM.liznumber,\n",
    "    y = dfM.groupby('liznumber').daysSinceCapture.max(),\n",
    "    name = 'males',\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = 'rgba(255, 182, 193, .9)',\n",
    "        opacity = 0.75,\n",
    "        line = dict(\n",
    "            width = 2,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [females, males]\n",
    "\n",
    "layout = dict(title = 'Days Since Initial Capture in Crystal Creek 2000 - 2017 By Sex',\n",
    "              yaxis = dict(\n",
    "                  title='Greatest Number of Days Since<br> Initial Capture',\n",
    "                  titlefont=dict(\n",
    "                      size=18)\n",
    "              ),\n",
    "              xaxis = dict(zeroline = False)\n",
    "             )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Days Since Initial Capture in Crystal Creek 2000 - 2017 By Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "males = go.Histogram(x = df_numbered.loc[(df_numbered.sex == 'm')& (df_numbered.species.isin(['j','v']))\n",
    "                                                                    ,'year']\n",
    "                     ,opacity= 0.75,name='males')\n",
    "females = go.Histogram(x = df_numbered.loc[(df_numbered.sex == 'f')& (df_numbered.species.isin(['j','v']))\n",
    "                                                                      ,'year']\n",
    "                       , opacity= 0.75, name = 'females')\n",
    "data = [males,females]\n",
    "py.iplot(data, filename = 'Distribution of Sex by Year in Crystal Creek 2000 - 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['liznumber','date','initialCaptureDate',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.year.value_counts(dropna=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QcCapture'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC of Capture Number and Recap Status\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "[Top Add Columns](#AddCol)\n",
    "\n",
    "[Top Capture Number](#capture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recapQuestion=df_numbered\\\n",
    ".loc[(df_numbered.capture==1 )&(df_numbered['new.recap']=='recap')&(df_numbered.species.isin(['j','v'])),:]\n",
    "print(\"There are {} instances in rows for which a lizard appears to have only one capture, \\\n",
    "but is listed as a recap.\\\n",
    "The distribution of these across years in the sample is as follows:\\n{}.\"\\\n",
    "      .format(recapQuestion.shape[0],recapQuestion.year.value_counts()))\n",
    "recapQuestion.to_csv(\"Questionable recaptures.csv\")#These individuals need to be rechecked in the raw notes\n",
    "recapQuestion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exportFinal'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Cleaned data\n",
    "[Top](#Table-of-Contents)\n",
    "\n",
    "Now we export the cleaned data to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numbered = df_numbered.rename(index = str, columns = {'new.recap':'newRecap'})\n",
    "qc_drop_cols = df_numbered.columns[df_numbered.columns.str.contains('force|drop')]\n",
    "df_full = df_numbered.drop(qc_drop_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(outputBig)\n",
    "timestamp = (pd.to_datetime('now')-pd.Timedelta(hours=4))\n",
    "timestamp = str(timestamp)[:-10].replace(':','hrs')+'min'\n",
    "filename = 'cleaned CC data 2000-2017_' + timestamp+ '.csv'\n",
    "df_full.to_csv(filename,index = False)\n",
    "filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
